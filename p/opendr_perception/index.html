<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>ROS Package: opendr_perception</title>
    <meta name="description" content="a community-maintained index of robotics software
">

    
    <link rel="canonical" href="http://index.rosdabbler.com/p/opendr_perception/">
    
    
    <link rel="icon" sizes="any" type="image/svg+xml" href="/assets/rosindex_logo.svg">

    

    <link rel="stylesheet" type="text/css" href="/bootstrap/css/bootstrap.min.css"/>
    <link rel="stylesheet" href="/css/main.css">
    

    

    <script type="text/javascript" src=/js/jquery.js></script>
    <script src=/bootstrap/js/bootstrap.min.js type="text/javascript"></script>
    <script src=/js/jquery-cookie.js type="text/javascript"></script>
    
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-EVD5Z6G6NH"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-EVD5Z6G6NH');
</script>

    <script type="text/javascript" src=/js/toc.js></script>

    <script src=/js/distro_switch.js></script>
  </head>

  <body>

    <header class="site-header">

  <div class="wrapper">
    <div class="container-fluid" style="margin-bottom: 10px">
      <div class="row">
        <!-- title -->
        <div class="col-xs-3" style="white-space:nowrap">
          <a class="site-title" href="/">
            <img src="/assets/rosindex_logo.svg" width="26" height="26" alt="ROS index logo" style="padding-bottom: 3px"/>
            ROS Index</a>
        </div>
        <!-- main internal links -->
        <div class="col-xs-6 text-center" style="padding:0px">
          <div class="btn-group hidden-xs" role="group" aria-label="..." style="padding: 6px">
            <div class="btn-group" role="group">
              <a href="/?search_packages=true" class="btn btn-default" role="button">Package List</a>
            </div>
            <div class="btn-group" role="group">
              <a href="/?search_repos=true" class="btn btn-default" role="button">Repository List</a>
            </div>
            <div class="btn-group" role="group">
              <a href="/search_deps" class="btn btn-default" role="button">System Dependencies</a>
            </div>
          </div>
          <div class="hidden-lg hidden-md hidden-sm">
            <button id="hLabel" class="btn btn-link dropdown-toggle" type="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
              Lists <span class="caret"></span>
            </button>
            <ul class="dropdown-menu" aria-labelledby="hLabel">
              <li><a href="/?search_packages=true">Package List</a></li>
              <li><a href="/?search_repos=true">Repository List</a></li>
              <li><a href="/search_deps">System Dependencies</a></li>
            </ul>
          </div>
        </div>
        <!-- additional links -->
        <div class="col-xs-3 text-right" style="white-space:nowrap; padding:0px">
          <ul class="list-inline" style="margin-bottom:0px;">
            <li class="dropdown hidden-xs hidden-sm">
              <button id="rLabel" class="btn btn-link" type="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                ROS Resources <span class="caret"></span>
              </button>
              <ul class="dropdown-menu" role="menu" aria-labelledby="rLabel">
                <li><a href="http://docs.ros.org/">Documentation</a></li>
                <li><a href="http://wiki.ros.org/Support">Support</a></li>
                <li><a href="http://discourse.ros.org/">Discussion Forum</a></li>
                <li><a href="http://status.ros.org/">Service Status</a></li>
                <li><a href="https://robotics.stackexchange.com/questions/tagged/ros">ros @ Robotics Stack Exchange</a></li>
                <li><a href="https://docs.ros.org/en/ros2_packages/">Package API</a></li>
              </ul>
            </li>
            <li class="dropdown hidden-xs hidden-sm">
              <button id="aLabel" class="btn btn-link" type="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                About <span class="caret"></span>
              </button>
              <ul class="dropdown-menu dropdown-menu-right" role="menu" aria-labelledby="aLabel">
                <li><a href="/about">About </a></li>
                <li><a href="/contribute">Contribute</a></li>
                <li><a href="/help">Help</a></li>
                <li><a href="/stats">Stats</a></li>
              </ul>
            </li>
            <li class="dropdown hidden-md hidden-lg">
              <button id="qLabel" class="btn btn-link" type="button"
                      data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                Resources <span class="caret"></span>
              </button>
              <ul class="dropdown-menu dropdown-menu-right" role="menu" aria-labelledby="qLabel">
                <li><a href="/about">About </a></li>
                <li><a href="/contribute">Contribute</a></li>
                <li><a href="/help">Help</a></li>
                <li><a href="/stats">Stats</a></li>
                <hr style="margin:7px" />
                <li><a href="http://docs.ros.org/">Documentation</a></li>
                <li><a href="http://wiki.ros.org/Support">Support</a></li>
                <li><a href="http://discourse.ros.org/">Discussion Forum</a></li>
                <li><a href="http://status.ros.org/">Service Status</a></li>
                <li><a href="https://robotics.stackexchange.com/questions/tagged/ros">ros @ Robotics Stack Exchange</a></li>
                <li><a href="https://docs.ros.org/en/ros2_packages/">Package API</a></li>
              </ul>
            </li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="container-fluid" style="margin-top: 20px">
  <div class="container-fluid">
    <div class="row">
      <ol class="breadcrumb">
        <li><a href="/">Home</a></li>
        <li><a href="/?search_packages=true">Packages</a></li>
        <li class="active">opendr_perception</li>
      </ol>
    </div>
    <div class="row">
      

<div id="distro-switch" class="btn-group btn-group-justified" data-toggle="buttons">
  
    <label id="humble-option" class="distro-button btn btn-xs btn-default" href="#humble" data="humble">
      <input type="radio" name="options" id="humble-radio" autocomplete="off"> humble
    </label>
  
    <label id="jazzy-option" class="distro-button btn btn-xs btn-default" href="#jazzy" data="jazzy">
      <input type="radio" name="options" id="jazzy-radio" autocomplete="off"> jazzy
    </label>
  
    <label id="kilted-option" class="distro-button btn btn-xs btn-default" href="#kilted" data="kilted">
      <input type="radio" name="options" id="kilted-radio" autocomplete="off"> kilted
    </label>
  
    <label id="rolling-option" class="distro-button btn btn-xs btn-default" href="#rolling" data="rolling">
      <input type="radio" name="options" id="rolling-radio" autocomplete="off"> rolling
    </label>
  
    <label id="github-option" class="distro-button btn btn-xs btn-primary" href="#github" data="github">
      <input type="radio" name="options" id="github-radio" autocomplete="off"> github
    </label>
  
    <label id="noetic-option" class="distro-button btn btn-xs btn-default" href="#noetic" data="noetic">
      <input type="radio" name="options" id="noetic-radio" autocomplete="off"> noetic
    </label>
  

  <!-- Older distros -->
  <div class="btn-group dropdown">
    <label type="button" class="btn btn-xs dropdown-toggle btn-default" data-toggle="dropdown" id="older-distro-button">
        <input type="radio" name="options" autocomplete="off">
      <span id="older-label">Older</span>
      <span class="caret"></span>
    </label>
    <ul class="dropdown-menu" role="menu">
      
        <li data="galactic" id="galactic-option" class="disabled older-distro-option"  href="#galactic">
          <a href="#galactic" data="galactic" id="galactic-button">galactic</a>
        </li>
      
        <li data="iron" id="iron-option" class="disabled older-distro-option"  href="#iron">
          <a href="#iron" data="iron" id="iron-button">iron</a>
        </li>
      
        <li data="melodic" id="melodic-option" class="disabled older-distro-option"  href="#melodic">
          <a href="#melodic" data="melodic" id="melodic-button">melodic</a>
        </li>
      
    </ul>
  </div>
</div>

    </div>
    <div class="row">
      &nbsp;
    </div>
  </div>
</div>


  <div class="distro distro-humble">
    <div class="container-fluid">
      
          <div class="alert alert-warning">No version for distro <strong>humble</strong>. Known supported distros are highlighted in the buttons above.</div>
      
    </div>
  </div>

  <div class="distro distro-jazzy">
    <div class="container-fluid">
      
          <div class="alert alert-warning">No version for distro <strong>jazzy</strong>. Known supported distros are highlighted in the buttons above.</div>
      
    </div>
  </div>

  <div class="distro distro-kilted">
    <div class="container-fluid">
      
          <div class="alert alert-warning">No version for distro <strong>kilted</strong>. Known supported distros are highlighted in the buttons above.</div>
      
    </div>
  </div>

  <div class="distro distro-rolling">
    <div class="container-fluid">
      
          <div class="alert alert-warning">No version for distro <strong>rolling</strong>. Known supported distros are highlighted in the buttons above.</div>
      
    </div>
  </div>

  <div class="distro distro-github">
    <div class="container-fluid">
      
        
        
        

        <div class="row">
          <div class="col-md-10">
            

<div class="well well-sm">
  
  <table class="table">
    <tr>
      <td width="100px" class="text-center">
        <img style="width: 80px" src="/assets/package.png">
      </td>
      <td>
        <h3 style="margin-top: 5px">
          <a style="text-decoration:none" href="/p/opendr_perception">opendr_perception</a> <small>package from <a style="text-decoration:none" href="/r/opendr/github-opendr-eu-opendr">opendr</a> repo</small>
        </h3>
          
            
            
              <a class="label label-primary pkg-label" href="/p/opendr_bridge">

opendr_bridge
</a>
            
          
            
            
              <a class="label label-primary pkg-label" href="/p/opendr_data_generation">

opendr_data_generation
</a>
            
          
            
            
              <span class="label label-default pkg-label">

opendr_perception
</span>
            
          
            
            
              <a class="label label-primary pkg-label" href="/p/opendr_planning">

opendr_planning
</a>
            
          
            
            
              <a class="label label-primary pkg-label" href="/p/opendr_simulation">

opendr_simulation
</a>
            
          
            
            
              <a class="label label-primary pkg-label" href="/p/opendr_interface">

opendr_interface
</a>
            
          
            
            
              <a class="label label-primary pkg-label" href="/p/mobile_manipulation_rl_demo">

mobile_manipulation_rl_demo
</a>
            
          
            
            
              <a class="label label-primary pkg-label" href="/p/franka_description">

franka_description
</a>
            
          
            
            
              <a class="label label-primary pkg-label" href="/p/panda_moveit_config">

panda_moveit_config
</a>
            
          
            
            
              <a class="label label-primary pkg-label" href="/p/single_demo_grasping_demo">

single_demo_grasping_demo
</a>
            
          
            
            
              <a class="label label-primary pkg-label" href="/p/fmp_slam_eval">

fmp_slam_eval
</a>
            
          
            
            
              <a class="label label-primary pkg-label" href="/p/map_simulator">

map_simulator
</a>
            
          
            
            
              <a class="label label-primary pkg-label" href="/p/openslam_gmapping">

openslam_gmapping
</a>
            
          
            
            
              <a class="label label-primary pkg-label" href="/p/gmapping">

gmapping
</a>
            
          
            
            
              <a class="label label-primary pkg-label" href="/p/slam_gmapping">

slam_gmapping
</a>
            
          
            
            
              <a class="label label-primary pkg-label" href="/p/mobile_manipulation_rl">

mobile_manipulation_rl
</a>
            
          
      </td>
    </tr>
  </table>
</div>

            <div class="visible-xs visible-sm">
              
              
              



<div class="list-group list-group-sm list-group-horizontal list-group-justified">
  <a class="list-group-item text-center"
     target="_blank"
     href="http://docs.ros.org/en/github/p/opendr_perception"
     title="View API documentation on docs.ros.org"
     >
       <span style="margin-right: 5px;" class="glyphicon glyphicon-file"></span>
       <br>
       API Docs
  </a>
  <a class="list-group-item text-center"
     target="_blank"
     href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception"
     title="View source code on repository">
       <span class="glyphicon glyphicon-folder-open" style="margin-right:5px;"></span>
       <br>
       Browse Code
  </a>
  
  
  
  
  
  
</div>

            </div>
            



<ul class="nav nav-tabs nav-justified" id="github-tabs" style="margin-bottom:20px">
  <li class="better-tabs active">
    <a href="#github-overview" data-toggle="tab">Overview</a>
  </li>
  <li class="better-tabs">
    
    
    
    
    
    <a href="#github-assets" data-toggle="tab"><span class="label label-default">0</span> Assets</a>
  </li>
  <li class="better-tabs">
    
    
    
    
    <a href="#github-deps" data-toggle="tab"><span class="label label-primary">9</span> Dependencies</a>
  </li>
  <li class="better-tabs">
    
    <a href="#github-tutorials" data-toggle="tab"><span class="label label-default">0</span> Tutorials</a>
  </li>
  <li class="better-tabs">
    <a href="#github-questions" data-toggle="tab"><span id="github-questions-count" class="label label-primary">0</span> Q & A</a>
  </li>
</ul>

<div class="tab-content">
  <div class="tab-pane active" id="github-overview">
    
    <div class="row">
      <div class="col-xs-12 col-sm-6 col-md-6 col-lg-6">
        <div class="panel panel-default">
          <div class="panel-heading"><h4 class="panel-title">Package Summary</h4></div>
          <div class="panel-body">
            <table class="table table-condensed">
              <tr>
                <td style="width:100px;" class="text-right"><strong>Tags</strong></td>
                <td>
                  
                  
                    <em>No category tags.</em>
                  
                </td>
              </tr>
              <tr>
                <td class="text-right"><strong>Version</strong></td>
                <td>3.0.0</td>
              </tr>
              <tr>
                <td class="text-right"><strong>License</strong></td>
                <td>Apache License v2.0</td>
              </tr>
              <tr>
                <td class="text-right"><strong>Build type</strong></td>
                <td>
                  
                    <span class="label label-default">
                  
                    AMENT_PYTHON</span>
                </td>
              </tr>
              <tr>
                <td class="text-right"><strong>Use</strong></td>
                <td>
                
                  <span class="label label-success">RECOMMENDED</span>
                
                </td>
              </tr>
            </table>
          </div>
          <div class="panel-heading"><h3 class="panel-title">Repository Summary</h3></div>
          <div class="panel-body">
            
            
            
  <link rel="stylesheet" href="/css/ci-status.css">
  <table class="table table-condensed">
    <tr>
      <td class="text-right"><b>Description</b></td>
      <td><span class="label label-default">A modular, open and non-proprietary toolkit for core robotic functionalities by harnessing deep learning</span></td>
    </tr>
    <tr>
      <td style="width:100px;" class="text-right"><b>Checkout URI</b></td>
      <td><a class="label label-default" href="https://github.com/opendr-eu/opendr.git">https://github.com/opendr-eu/opendr.git</a></td>
    </tr>
    <tr>
      <td class="text-right"><b>VCS Type</b></td>
      <td><span class="label label-default">git</span></td>
    </tr>
    <tr>
      <td class="text-right"><b>VCS Version</b></td>
      <td><span class="label label-default">master</span></td>
    </tr>
    <tr>
      <td style="white-space: nowrap;" class="text-right"><b>Last Updated</b></div>
      <td>
        
          <span class="label label-default"><span class="glyphicon glyphicon-time"></span> 2025-01-29
        </span>
      </td>
    </tr>
    <tr>
      <td class="text-right"><b>Dev Status</b></td>
      <td>
        
          <span class="label label-warning">UNKNOWN
        </span>
      </td>
    </tr>
              <tr>
                <td class="text-right"><b>CI status</b></td>
                <td class="ci-status">
                  
                  
                    <span class="label label-default" title="No CI information available for this package.">No Continuous Integration</span>
                  
                </td>
              </tr>
    <tr>
      <td class="text-right"><b>Released</b></td>
      <td>
        
          <span class="label label-default">UNRELEASED
        </span>
      </td>
    </tr>
    <tr>
      <td class="text-right"><b>Tags</b></td>
      <td>
        
        
          <span class="label label-default">deep-learning</span> <span class="label label-default">robotics</span> 
        
      </td>
    </tr>
    <tr>
      <td class="text-right"><b>Contributing</b></td>
      <td>
        <a class="label label-primary" href="/r/opendr/#github-contribute-lists-help-wanted">
          Help Wanted (<span class="contribute-lists-help-wanted-count">0</span>)
        </a>
        <br>
        <a class="label label-primary" href="/r/opendr/#github-contribute-lists-good-first-issue">
          Good First Issues (<span class="contribute-lists-good-first-issue-count">0</span>)
        </a>
        <br>
        <a class="label label-primary" href="/r/opendr/#github-contribute-lists-pull-requests">
          Pull Requests to Review (<span class="contribute-lists-pull-requests-count">0</span>)
        </a>
      </td>
    </tr>
  </table>

          </div>
        </div>
      </div>
      <div class="col-xs-12 col-sm-6 col-md-6 col-lg-6">
        <div class="panel panel-default">
          <div class="panel-heading"><h3 class="panel-title">Package Description</h3></div>
          <div class="panel-body">
            OpenDR ROS2 nodes for the perception package
          </div>
          <div class="panel-heading"><h4 class="panel-title">Additional Links</h4></div>
          <div class="panel-body">
            
            
              <em>No additional links.</em>
            
          </div>
          <div class="panel-heading"><h4 class="panel-title">Maintainers</h4></div>
          <div class="panel-body">
            
            
              <ul class="list-unstyled">
                
                  <li>OpenDR Project Coordinator</li>
                
              </ul>
            
          </div>
          <div class="panel-heading"><h4 class="panel-title">Authors</h4></div>
          <div class="panel-body">
            
            
              <em>No additional authors.</em>
            
          </div>
        </div>
      </div>
    </div>
    
      
        <div class="row">
          <div class="col-xs-12">
            <div class="panel panel-default">
              <div class="panel-heading"><span class="glyphicon glyphicon-file"></span><a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/README.md" title="Open in git repository">projects/opendr_ws_2/src/opendr_perception/README.md</a></div>
              <div class="panel-body">
                    <div class="rendered-markdown">
<h1 id="opendr-perception-package">OpenDR Perception Package</h1>

<p>This package contains ROS2 nodes related to the perception package of OpenDR.</p>

<hr>

<h2 id="prerequisites">Prerequisites</h2>

<p>Before you can run any of the toolkit’s ROS2 nodes, some prerequisites need to be fulfilled:</p>
<ol>
  <li>First of all, you need to <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../README.md#first-time-setup">set up the required packages and build your workspace.</a>
</li>
  <li>
    <p><em>(Optional for nodes with <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#rgb-input-nodes">RGB input</a>)</em></p>

    <p>For basic usage and testing, all the toolkit’s ROS2 nodes that use RGB images are set up to expect input from a basic webcam using the default package <code class="language-plaintext highlighter-rouge">usb_cam</code> which is installed with OpenDR. You can run the webcam node in a new terminal:</p>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ros2 run usb_cam usb_cam_node_exe
    
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>By default, the USB cam node publishes images on `/image_raw` and the RGB input nodes subscribe to this topic if not provided with an input topic argument. 
As explained for each node below, you can modify the topics via arguments, so if you use any other node responsible for publishing images, **make sure to change the input topic accordingly.**
</code></pre></div></div>

<ol>
  <li>
    <p><em>(Optional for nodes with <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#audio-input">audio input</a> or <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#rgb--audio-input">audiovisual input</a>)</em></p>

    <p>For basic usage and testing, the toolkit’s ROS2 nodes that use audio as input are set up to expect input from a basic audio device using the default package <code class="language-plaintext highlighter-rouge">audio_common</code>  which is installed with OpenDR. You can run the audio node in a new terminal:</p>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ros2 launch audio_capture capture_wave.launch.xml
    
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>By default, the audio capture node publishes audio data on `/audio/audio` and the audio input nodes subscribe to this topic if not provided with an input topic argument. 
As explained for each node below, you can modify the topics via arguments, so if you use any other node responsible for publishing audio, **make sure to change the input topic accordingly.**
</code></pre></div></div>

<hr>

<h2 id="notes">Notes</h2>

<ul>
  <li>
    <h3 id="display-output-images-with-rqt_image_view">Display output images with rqt_image_view</h3>
    <p>For any node that outputs images, <code class="language-plaintext highlighter-rouge">rqt_image_view</code> can be used to display them by running the following command:</p>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ros2 run rqt_image_view rqt_image_view &amp;
    
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>A window will appear, where the topic that you want to view can be selected from the drop-down menu on the top-left area of the window.
Refer to each node's documentation below to find out the default output image topic, where applicable, and select it on the drop-down menu of rqt_image_view.
</code></pre></div></div>

<ul>
  <li>
    <h3 id="echo-node-output">Echo node output</h3>
    <p>All OpenDR nodes publish some kind of detection message, which can be echoed by running the following command:</p>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ros2 topic <span class="nb">echo</span> /opendr/topic_name
    
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>You can find out the default topic name for each node, in its documentation below.
</code></pre></div></div>

<ul>
  <li>
    <h3 id="increase-performance-by-disabling-output">Increase performance by disabling output</h3>
    <p>Optionally, nodes can be modified via command line arguments, which are presented for each node separately below.
  Generally, arguments give the option to change the input and output topics, the device the node runs on (CPU or GPU), etc.
  When a node publishes on several topics, where applicable, a user can opt to disable one or more of the outputs by providing <code class="language-plaintext highlighter-rouge">None</code> in the corresponding output topic.
  This disables publishing on that topic, forgoing some operations in the node, which might increase its performance.</p>

    <p><em>An example would be to disable the output annotated image topic in a node when visualization is not needed and only use the detection message in another node, thus eliminating the OpenCV operations.</em></p>
  </li>
  <li>
    <h3 id="logging-the-node-performance-in-the-console">Logging the node performance in the console</h3>
    <p>OpenDR provides the utility <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#performance-ros2-node">performance node</a> to log performance messages in the console for the running node.
 You can set the <code class="language-plaintext highlighter-rouge">performance_topic</code> of the node you are using and also run the performance node to get the time it takes for the
 node to process a single input and its average speed expressed in frames per second.</p>
  </li>
  <li>
    <h3 id="an-example-diagram-of-opendr-nodes-running">An example diagram of OpenDR nodes running</h3>
    <p><img src="https://raw.githubusercontent.com/opendr-eu/opendr/master/projects/opendr_ws_2/src/opendr_perception/../../images/opendr_node_diagram.png" alt="Face Detection ROS2 node running diagram"></p>
    <ul>
      <li>On the left, the <code class="language-plaintext highlighter-rouge">usb_cam</code> node can be seen, which is using a system camera to publish images on the <code class="language-plaintext highlighter-rouge">/image_raw</code> topic.</li>
      <li>In the middle, OpenDR’s face detection node is running taking as input the published image. By default, the node has its input topic set to <code class="language-plaintext highlighter-rouge">/image_raw</code>.</li>
      <li>To the right the two output topics of the face detection node can be seen.
  The bottom topic <code class="language-plaintext highlighter-rouge">/opendr/image_faces_annotated</code> is the annotated image which can be easily viewed with <code class="language-plaintext highlighter-rouge">rqt_image_view</code> as explained earlier.
  The other topic <code class="language-plaintext highlighter-rouge">/opendr/faces</code> is the detection message which contains the detected faces’ detailed information.
  This message can be easily viewed by running <code class="language-plaintext highlighter-rouge">ros2 topic echo /opendr/faces</code> in a terminal.</li>
    </ul>
  </li>
</ul>

<!-- - ### Other notes -->

<hr>

<h2 id="rgb-input-nodes">RGB input nodes</h2>

<h3 id="pose-estimation-ros2-node">Pose Estimation ROS2 Node</h3>

<p>You can find the pose estimation ROS2 node python script <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/pose_estimation_node.py">here</a> to inspect the code and modify it as you wish to fit your needs.
The node makes use of the toolkit’s <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/pose_estimation/lightweight_open_pose/lightweight_open_pose_learner.py">pose estimation tool</a> whose documentation can be found <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/lightweight-open-pose.md">here</a>.
The node publishes the detected poses in <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../opendr_interface/msg/OpenDRPose2D.msg">OpenDR’s 2D pose message format</a>, which saves a list of <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../opendr_interface/msg/OpenDRPose2DKeypoint.msg">OpenDR’s keypoint message format</a>.</p>

<h4 id="instructions-for-basic-usage">Instructions for basic usage:</h4>

<ol>
  <li>
    <p>Start the node responsible for publishing images. If you have a USB camera, then you can use the <code class="language-plaintext highlighter-rouge">usb_cam_node</code> as explained in the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#prerequisites">prerequisites above</a>.</p>
  </li>
  <li>
    <p>You are then ready to start the pose detection node:</p>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ros2 run opendr_perception pose_estimation
    
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The following optional arguments are available:    - `-h, --help`: show a help message and exit    - `-i or --input_rgb_image_topic INPUT_RGB_IMAGE_TOPIC`: topic name for input RGB image (default=`/image_raw`)    - `-o or --output_rgb_image_topic OUTPUT_RGB_IMAGE_TOPIC`: topic name for output annotated RGB image, `None` to stop the node from publishing on this topic (default=`/opendr/image_pose_annotated`)    - `-d or --detections_topic DETECTIONS_TOPIC`: topic name for detection messages, `None` to stop the node from publishing on this topic (default=`/opendr/poses`)    - `--performance_topic PERFORMANCE_TOPIC`: topic name for performance messages (default=`None`, disabled)    - `--device DEVICE`: Device to use, either `cpu` or `cuda`, falls back to `cpu` if GPU or CUDA is not found (default=`cuda`)    - `--accelerate`: Acceleration flag that causes pose estimation to run faster but with less accuracy
</code></pre></div></div>

<ol>
  <li>Default output topics:
    <ul>
      <li>Output images: <code class="language-plaintext highlighter-rouge">/opendr/image_pose_annotated</code>
</li>
      <li>Detection messages: <code class="language-plaintext highlighter-rouge">/opendr/poses</code>
</li>
    </ul>

    <p>For viewing the output, refer to the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#notes">notes above.</a></p>
  </li>
</ol>

<h3 id="high-resolution-pose-estimation-ros2-node">High Resolution Pose Estimation ROS2 Node</h3>

<p>You can find the high resolution pose estimation ROS2 node python script <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/hr_pose_estimation_node.py">here</a> to inspect the code and modify it as you wish to fit your needs.
The node makes use of the toolkit’s <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/pose_estimation/hr_pose_estimation/high_resolution_learner.py">high resolution pose estimation tool</a> whose documentation can be found <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/high-resolution-pose-estimation.md">here</a>.
The node publishes the detected poses in <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../opendr_interface/msg/OpenDRPose2D.msg">OpenDR’s 2D pose message format</a>, which saves a list of <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../opendr_interface/msg/OpenDRPose2DKeypoint.msg">OpenDR’s keypoint message format</a>.</p>

<h4 id="instructions-for-basic-usage-1">Instructions for basic usage:</h4>

<ol>
  <li>
    <p>Start the node responsible for publishing images. If you have a USB camera, then you can use the <code class="language-plaintext highlighter-rouge">usb_cam_node</code> as explained in the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#prerequisites">prerequisites above</a>.</p>
  </li>
  <li>
    <p>You are then ready to start the high resolution pose detection node:</p>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ros2 run opendr_perception hr_pose_estimation
    
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The following optional arguments are available:    - `-h, --help`: show a help message and exit    - `-i or --input_rgb_image_topic INPUT_RGB_IMAGE_TOPIC`: topic name for input RGB image (default=`/image_raw`)    - `-o or --output_rgb_image_topic OUTPUT_RGB_IMAGE_TOPIC`: topic name for output annotated RGB image, `None` to stop the node from publishing on this topic (default=`/opendr/image_pose_annotated`)    - `-d or --detections_topic DETECTIONS_TOPIC`: topic name for detection messages, `None` to stop the node from publishing on this topic (default=`/opendr/poses`)    - `--performance_topic PERFORMANCE_TOPIC`: topic name for performance messages (default=`None`, disabled)    - `--device DEVICE`: Device to use, either `cpu` or `cuda`, falls back to `cpu` if GPU or CUDA is not found (default=`cuda`)    - `--accelerate`: Acceleration flag that causes pose estimation to run faster but with less accuracy
</code></pre></div></div>

<ol>
  <li>Default output topics:
    <ul>
      <li>Output images: <code class="language-plaintext highlighter-rouge">/opendr/image_pose_annotated</code>
</li>
      <li>Detection messages: <code class="language-plaintext highlighter-rouge">/opendr/poses</code>
</li>
    </ul>

    <p>For viewing the output, refer to the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#notes">notes above.</a></p>
  </li>
</ol>

<h3 id="fall-detection-ros2-node">Fall Detection ROS2 Node</h3>

<p>You can find the fall detection ROS2 node python script <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/fall_detection_node.py">here</a> to inspect the code and modify it as you wish to fit your needs.
The node makes use of the toolkit’s <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/fall_detection/fall_detector_learner.py">fall detection tool</a> whose documentation can be found <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/fall-detection.md">here</a>.
Fall detection is rule-based and works on top of pose estimation.</p>

<p>This node normally runs on <code class="language-plaintext highlighter-rouge">detection mode</code> where it subscribes to a topic of OpenDR poses and detects whether the poses are fallen persons or not.
By providing an image topic the node runs on <code class="language-plaintext highlighter-rouge">visualization mode</code>. It also gets images, performs pose estimation internally and visualizes the output on an output image topic.
Note that when providing an image topic the node has significantly worse performance in terms of speed, due to running pose estimation internally.</p>

<ul>
  <li>
    <h4 id="instructions-for-basic-usage-in-detection-mode">Instructions for basic usage in <code class="language-plaintext highlighter-rouge">detection mode</code>:</h4>
  </li>
</ul>

<ol>
  <li>
    <p>Start the node responsible for publishing poses. Refer to the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#pose-estimation-ros2-node">pose estimation node above</a>.</p>
  </li>
  <li>
    <p>You are then ready to start the fall detection node:</p>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ros2 run opendr_perception fall_detection
    
</code></pre></div></div>
<p>The following optional arguments are available and relevant for running fall detection on pose messages only:</p>
<ul>
  <li>
<code class="language-plaintext highlighter-rouge">-h or --help</code>: show a help message and exit</li>
  <li>
<code class="language-plaintext highlighter-rouge">-ip or --input_pose_topic INPUT_POSE_TOPIC</code>: topic name for input pose, <code class="language-plaintext highlighter-rouge">None</code> to stop the node from running detections on pose messages (default=<code class="language-plaintext highlighter-rouge">/opendr/poses</code>)</li>
  <li>
<code class="language-plaintext highlighter-rouge">-d or --detections_topic DETECTIONS_TOPIC</code>: topic name for detection messages (default=<code class="language-plaintext highlighter-rouge">/opendr/fallen</code>)</li>
  <li>
<code class="language-plaintext highlighter-rouge">--performance_topic PERFORMANCE_TOPIC</code>: topic name for performance messages, note that performance will be published to <code class="language-plaintext highlighter-rouge">PERFORMANCE_TOPIC/fallen</code> (default=<code class="language-plaintext highlighter-rouge">None</code>, disabled)</li>
</ul>

<ol>
  <li>Detections are published on the <code class="language-plaintext highlighter-rouge">detections_topic</code>
</li>
</ol>

<ul>
  <li>
    <h4 id="instructions-for-visualization-mode">Instructions for <code class="language-plaintext highlighter-rouge">visualization mode</code>:</h4>
  </li>
</ul>

<ol>
  <li>
    <p>Start the node responsible for publishing images. If you have a USB camera, then you can use the <code class="language-plaintext highlighter-rouge">usb_cam_node</code> as explained in the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#prerequisites">prerequisites above</a>.</p>
  </li>
  <li>
    <p>You are then ready to start the fall detection node in <code class="language-plaintext highlighter-rouge">visualization mode</code>, which needs an input image topic to be provided:</p>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ros2 run opendr_perception fall_detection <span class="nt">-ii</span> /image_raw
    
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The following optional arguments are available and relevant for running fall detection on images. Note that the `input_rgb_image_topic` is required for running in `visualization mode`:    - `-h or --help`: show a help message and exit    - `-ii or --input_rgb_image_topic INPUT_RGB_IMAGE_TOPIC`: topic name for input RGB image (default=`None`)    - `-o or --output_rgb_image_topic OUTPUT_RGB_IMAGE_TOPIC`: topic name for output annotated RGB image (default=`/opendr/image_fallen_annotated`)    - `-d or --detections_topic DETECTIONS_TOPIC`: topic name for detection messages (default=`/opendr/fallen`)    - `--performance_topic PERFORMANCE_TOPIC`: topic name for performance messages, note that performance will be published to `PERFORMANCE_TOPIC/image` (default=`None`, disabled)    - `--device DEVICE`: device to use, either `cpu` or `cuda`, falls back to `cpu` if GPU or CUDA is not found (default=`cuda`)    - `--accelerate`: acceleration flag that causes pose estimation that runs internally to run faster but with less accuracy
</code></pre></div></div>

<ul>
  <li>Default output topics:
    <ul>
      <li>Detection messages: <code class="language-plaintext highlighter-rouge">/opendr/fallen</code>
</li>
      <li>Output images: <code class="language-plaintext highlighter-rouge">/opendr/image_fallen_annotated</code>
</li>
    </ul>

    <p>For viewing the output, refer to the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#notes">notes above.</a></p>
  </li>
</ul>

<p><strong>Notes</strong></p>

<p>Note that when the node runs on the default <code class="language-plaintext highlighter-rouge">detection mode</code> it is significantly faster than when it is provided with an 
input image topic. However, pose estimation needs to be performed externally on another node which publishes poses.
When an input image topic is provided and the node runs in <code class="language-plaintext highlighter-rouge">visualization mode</code>, it runs pose estimation internally, and 
consequently it is recommended to only use it for testing purposes and not run other pose estimation nodes in parallel.
The node can run in both modes in parallel or only on one of the two. To run the node only on <code class="language-plaintext highlighter-rouge">visualization mode</code> provide
the argument <code class="language-plaintext highlighter-rouge">-ip None</code> to disable the <code class="language-plaintext highlighter-rouge">detection mode</code>. Detection messages on <code class="language-plaintext highlighter-rouge">detections_topic</code> are published in both modes.</p>

<h3 id="wave-detection-ros2-node">Wave Detection ROS2 Node</h3>

<p>You can find the wave detection ROS2 node python script <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/wave_detection_node.py">here</a> to inspect the code and modify it as you wish to fit your needs.
The node is based on a <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../projects/python/perception/pose_estimation/lightweight_open_pose/demos/wave_detection_demo.py">wave detection demo of the Lightweight OpenPose tool</a>.
Wave detection is rule-based and works on top of pose estimation.</p>

<p>This node normally runs on <code class="language-plaintext highlighter-rouge">detection mode</code> where it subscribes to a topic of OpenDR poses and detects whether the poses are waving or not.
By providing an image topic the node runs on <code class="language-plaintext highlighter-rouge">visualization mode</code>. It also gets images, performs pose estimation internally and visualizes the output on an output image topic.
Note that when providing an image topic the node has significantly worse performance in terms of speed, due to running pose estimation internally.</p>

<ul>
  <li>
    <h4 id="instructions-for-basic-usage-in-detection-mode-1">Instructions for basic usage in <code class="language-plaintext highlighter-rouge">detection mode</code>:</h4>
  </li>
</ul>

<ol>
  <li>
    <p>Start the node responsible for publishing poses. Refer to the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#pose-estimation-ros2-node">pose estimation node above</a>.</p>
  </li>
  <li>
    <p>You are then ready to start the wave detection node:</p>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ros2 run opendr_perception wave_detection
    
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The following optional arguments are available and relevant for running fall detection on pose messages only:    - `-h or --help`: show a help message and exit    - `-ip or --input_pose_topic INPUT_POSE_TOPIC`: topic name for input pose, `None` to stop the node from running detections on pose messages (default=`/opendr/poses`)    - `-d or --detections_topic DETECTIONS_TOPIC`: topic name for detection messages (default=`/opendr/wave`)    - `--performance_topic PERFORMANCE_TOPIC`: topic name for performance messages, note that performance will be published to `PERFORMANCE_TOPIC/wave` (default=`None`, disabled)
</code></pre></div></div>

<ol>
  <li>Detections are published on the <code class="language-plaintext highlighter-rouge">detections_topic</code>
</li>
</ol>

<ul>
  <li>
    <h4 id="instructions-for-visualization-mode-1">Instructions for <code class="language-plaintext highlighter-rouge">visualization mode</code>:</h4>
  </li>
</ul>

<ol>
  <li>
    <p>Start the node responsible for publishing images. If you have a USB camera, then you can use the <code class="language-plaintext highlighter-rouge">usb_cam_node</code> as explained in the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#prerequisites">prerequisites above</a>.</p>
  </li>
  <li>
    <p>You are then ready to start the wave detection node in <code class="language-plaintext highlighter-rouge">visualization mode</code>, which needs an input image topic to be provided:</p>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ros2 run opendr_perception wave_detection <span class="nt">-ii</span> /image_raw
    
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The following optional arguments are available and relevant for running wave detection on images. Note that the `input_rgb_image_topic` is required for running in `visualization mode`:    - `-h or --help`: show a help message and exit    - `-ii or --input_rgb_image_topic INPUT_RGB_IMAGE_TOPIC`: topic name for input RGB image (default=`None`)    - `-o or --output_rgb_image_topic OUTPUT_RGB_IMAGE_TOPIC`: topic name for output annotated RGB image (default=`/opendr/image_wave_annotated`)    - `-d or --detections_topic DETECTIONS_TOPIC`: topic name for detection messages (default=`/opendr/wave`)    - `--performance_topic PERFORMANCE_TOPIC`: topic name for performance messages, note that performance will be published to `PERFORMANCE_TOPIC/image` (default=`None`, disabled)    - `--device DEVICE`: device to use, either `cpu` or `cuda`, falls back to `cpu` if GPU or CUDA is not found (default=`cuda`)    - `--accelerate`: acceleration flag that causes pose estimation that runs internally to run faster but with less accuracy
</code></pre></div></div>

<ul>
  <li>Default output topics:
    <ul>
      <li>Detection messages: <code class="language-plaintext highlighter-rouge">/opendr/wave</code>
</li>
      <li>Output images: <code class="language-plaintext highlighter-rouge">/opendr/image_wave_annotated</code>
</li>
    </ul>

    <p>For viewing the output, refer to the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#notes">notes above.</a></p>
  </li>
</ul>

<p><strong>Notes</strong></p>

<p>Note that when the node runs on the default <code class="language-plaintext highlighter-rouge">detection mode</code> it is significantly faster than when it is provided with an 
input image topic. However, pose estimation needs to be performed externally on another node which publishes poses.
When an input image topic is provided and the node runs in <code class="language-plaintext highlighter-rouge">visualization mode</code>, it runs pose estimation internally, and 
consequently it is recommended to only use it for testing purposes and not run other pose estimation nodes in parallel.
The node can run in both modes in parallel or only on one of the two. To run the node only on <code class="language-plaintext highlighter-rouge">visualization mode</code> provide
the argument <code class="language-plaintext highlighter-rouge">-ip None</code> to disable the <code class="language-plaintext highlighter-rouge">detection mode</code>. Detection messages on <code class="language-plaintext highlighter-rouge">detections_topic</code> are published in both modes.</p>

<h3 id="face-detection-ros2-node">Face Detection ROS2 Node</h3>

<p>The face detection ROS2 node supports both the ResNet and MobileNet versions, the latter of which performs masked face detection as well.</p>

<p>You can find the face detection ROS2 node python script <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/face_detection_retinaface_node.py">here</a> to inspect the code and modify it as you wish to fit your needs.
The node makes use of the toolkit’s <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/object_detection_2d/retinaface/retinaface_learner.py">face detection tool</a> whose documentation can be found <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/face-detection-2d-retinaface.md">here</a>.</p>

<h4 id="instructions-for-basic-usage-2">Instructions for basic usage:</h4>

<ol>
  <li>
    <p>Start the node responsible for publishing images. If you have a USB camera, then you can use the <code class="language-plaintext highlighter-rouge">usb_cam_node</code> as explained in the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#prerequisites">prerequisites above</a>.</p>
  </li>
  <li>
    <p>You are then ready to start the face detection node</p>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ros2 run opendr_perception face_detection_retinaface
    
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The following optional arguments are available:    - `-h or --help`: show a help message and exit    - `-i or --input_rgb_image_topic INPUT_RGB_IMAGE_TOPIC`: topic name for input RGB image (default=`/image_raw`)    - `-o or --output_rgb_image_topic OUTPUT_RGB_IMAGE_TOPIC`: topic name for output annotated RGB image, `None` to stop the node from publishing on this topic (default=`/opendr/image_faces_annotated`)    - `-d or --detections_topic DETECTIONS_TOPIC`: topic name for detection messages, `None` to stop the node from publishing on this topic (default=`/opendr/faces`)    - `--performance_topic PERFORMANCE_TOPIC`: topic name for performance messages (default=`None`, disabled)    - `--device DEVICE`: device to use, either `cpu` or `cuda`, falls back to `cpu` if GPU or CUDA is not found (default=`cuda`)    - `--backbone BACKBONE`: retinaface backbone, options are either `mnet` or `resnet`, where `mnet` detects masked faces as well (default=`resnet`)
</code></pre></div></div>

<ol>
  <li>Default output topics:
    <ul>
      <li>Output images: <code class="language-plaintext highlighter-rouge">/opendr/image_faces_annotated</code>
</li>
      <li>Detection messages: <code class="language-plaintext highlighter-rouge">/opendr/faces</code>
</li>
    </ul>

    <p>For viewing the output, refer to the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#notes">notes above.</a></p>
  </li>
</ol>

<h3 id="face-recognition-ros2-node">Face Recognition ROS2 Node</h3>

<p>You can find the face recognition ROS2 node python script <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/face_recognition_node.py">here</a> to inspect the code and modify it as you wish to fit your needs.
The node makes use of the toolkit’s <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/face_recognition/face_recognition_learner.py">face recognition tool</a> whose documentation can be found <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/face-recognition.md">here</a>.</p>

<h4 id="instructions-for-basic-usage-3">Instructions for basic usage:</h4>

<ol>
  <li>
    <p>Start the node responsible for publishing images. If you have a USB camera, then you can use the <code class="language-plaintext highlighter-rouge">usb_cam_node</code> as explained in the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#prerequisites">prerequisites above</a>.</p>
  </li>
  <li>
    <p>You are then ready to start the face recognition node:</p>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ros2 run opendr_perception face_recognition
    
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The following optional arguments are available:    - `-h or --help`: show a help message and exit    - `-i or --input_rgb_image_topic INPUT_RGB_IMAGE_TOPIC`: topic name for input RGB image (default=`/image_raw`)    - `-o or --output_rgb_image_topic OUTPUT_RGB_IMAGE_TOPIC`: topic name for output annotated RGB image, `None` to stop the node from publishing on this topic (default=`/opendr/image_face_reco_annotated`)    - `-d or --detections_topic DETECTIONS_TOPIC`: topic name for detection messages, `None` to stop the node from publishing on this topic (default=`/opendr/face_recognition`)    - `-id or --detections_id_topic DETECTIONS_ID_TOPIC`: topic name for detection ID messages, `None` to stop the node from publishing on this topic (default=`/opendr/face_recognition_id`)    - `--performance_topic PERFORMANCE_TOPIC`: topic name for performance messages (default=`None`, disabled)    - `--device DEVICE`: device to use, either `cpu` or `cuda`, falls back to `cpu` if GPU or CUDA is not found (default=`cuda`)    - `--backbone BACKBONE`: backbone network (default=`mobilefacenet`)    - `--dataset_path DATASET_PATH`: path of the directory where the images of the faces to be recognized are stored (default=`./database`)
</code></pre></div></div>

<ol>
  <li>Default output topics:
    <ul>
      <li>Output images: <code class="language-plaintext highlighter-rouge">/opendr/image_face_reco_annotated</code>
</li>
      <li>Detection messages: <code class="language-plaintext highlighter-rouge">/opendr/face_recognition</code> and <code class="language-plaintext highlighter-rouge">/opendr/face_recognition_id</code>
</li>
    </ul>

    <p>For viewing the output, refer to the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#notes">notes above.</a></p>
  </li>
</ol>

<p><strong>Notes</strong></p>

<p>Reference images should be placed in a defined structure like:</p>
<ul>
  <li>imgs
    <ul>
      <li>ID1
        <ul>
          <li>image1</li>
          <li>image2</li>
        </ul>
      </li>
      <li>ID2</li>
      <li>ID3</li>
      <li>…</li>
    </ul>
  </li>
</ul>

<p>The default dataset path is <code class="language-plaintext highlighter-rouge">./database</code>. Please use the <code class="language-plaintext highlighter-rouge">--database_path ./your/path/</code> argument to define a custom one.
Τhe name of the sub-folder, e.g. ID1, will be published under <code class="language-plaintext highlighter-rouge">/opendr/face_recognition_id</code>.</p>

<p>The database entry and the returned confidence is published under the topic name <code class="language-plaintext highlighter-rouge">/opendr/face_recognition</code>, and the human-readable ID
under <code class="language-plaintext highlighter-rouge">/opendr/face_recognition_id</code>.</p>

<h3 id="2d-object-detection-ros2-nodes">2D Object Detection ROS2 Nodes</h3>

<p>For 2D object detection, there are several ROS2 nodes implemented using various algorithms. The generic object detectors are SSD, YOLOv3, YOLOv5, CenterNet, Nanodet and DETR.</p>

<p>You can find the 2D object detection ROS2 node python scripts here:
<a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/object_detection_2d_ssd_node.py">SSD node</a>, <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/object_detection_2d_yolov3_node.py">YOLOv3 node</a>, <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/object_detection_2d_yolov5_node.py">YOLOv5 node</a>, <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/object_detection_2d_centernet_node.py">CenterNet node</a>, <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/object_detection_2d_nanodet_node.py">Nanodet node</a> and <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/object_detection_2d_detr_node.py">DETR node</a>,
where you can inspect the code and modify it as you wish to fit your needs.
The nodes makes use of the toolkit’s various 2D object detection tools:
<a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/object_detection_2d/ssd/ssd_learner.py">SSD tool</a>, <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/object_detection_2d/yolov3/yolov3_learner.py">YOLOv3 tool</a>, <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/object_detection_2d/yolov5/yolov5_learner.py">YOLOv5 tool</a>,
<a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/object_detection_2d/centernet/centernet_learner.py">CenterNet tool</a>, <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/object_detection_2d/nanodet/nanodet_learner.py">Nanodet tool</a>, <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/object_detection_2d/detr/detr_learner.py">DETR tool</a>,
whose documentation can be found here:
<a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/object-detection-2d-ssd.md">SSD docs</a>, <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/object-detection-2d-yolov3.md">YOLOv3 docs</a>, <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/object-detection-2d-yolov5.md">YOLOv5 docs</a>,
<a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/object-detection-2d-centernet.md">CenterNet docs</a>, <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/nanodet.md">Nanodet docs</a>, <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/detr.md">DETR docs</a>.</p>

<p>Note that the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#semantic-segmentation-yolov8-ros-node">semantic segmentation YOLOv8 node</a> can also perform 2D object detection.</p>

<h4 id="instructions-for-basic-usage-4">Instructions for basic usage:</h4>

<ol>
  <li>
    <p>Start the node responsible for publishing images. If you have a USB camera, then you can use the <code class="language-plaintext highlighter-rouge">usb_cam_node</code> as explained in the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#prerequisites">prerequisites above</a>.</p>
  </li>
  <li>
    <p>You are then ready to start a 2D object detector node:</p>
    <ol>
      <li>SSD node</li>
    </ol>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      ros2 run opendr_perception object_detection_2d_ssd
      
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  The following optional arguments are available for the SSD node:
  - `--backbone BACKBONE`: Backbone network (default=`vgg16_atrous`)
  - `--nms_type NMS_TYPE`: Non-Maximum Suppression type options are `default`, `seq2seq-nms`, `soft-nms`, `fast-nms`, `cluster-nms` (default=`default`)
</code></pre></div></div>

<ol>
  <li>YOLOv3 node</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      ros2 run opendr_perception object_detection_2d_yolov3
      
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  The following optional argument is available for the YOLOv3 node:
  - `--backbone BACKBONE`: Backbone network (default=`darknet53`)
</code></pre></div></div>

<ol>
  <li>YOLOv5 node</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      ros2 run opendr_perception object_detection_2d_yolov5
      
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  The following optional argument is available for the YOLOv5 node:
  - `--model_name MODEL_NAME`: Network architecture, options are `yolov5s`, `yolov5n`, `yolov5m`, `yolov5l`, `yolov5x`, `yolov5n6`, `yolov5s6`, `yolov5m6`, `yolov5l6`, `custom` (default=`yolov5s`)
</code></pre></div></div>

<ol>
  <li>CenterNet node</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      ros2 run opendr_perception object_detection_2d_centernet
      
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  The following optional argument is available for the CenterNet node:
  - `--backbone BACKBONE`: Backbone network (default=`resnet50_v1b`)
</code></pre></div></div>

<ol>
  <li>Nanodet node</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      ros2 run opendr_perception object_detection_2d_nanodet
      
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  The following optional argument is available for the Nanodet node:
  - `--model Model`: Model that config file will be used (default=`plus_m_1.5x_416`)
</code></pre></div></div>

<ol>
  <li>DETR node</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      ros2 run opendr_perception object_detection_2d_detr
      
</code></pre></div></div>

<p>The following optional arguments are available for all nodes above:</p>
<ul>
  <li>
<code class="language-plaintext highlighter-rouge">-h or --help</code>: show a help message and exit</li>
  <li>
<code class="language-plaintext highlighter-rouge">-i or --input_rgb_image_topic INPUT_RGB_IMAGE_TOPIC</code>: topic name for input RGB image (default=<code class="language-plaintext highlighter-rouge">/image_raw</code>)</li>
  <li>
<code class="language-plaintext highlighter-rouge">-o or --output_rgb_image_topic OUTPUT_RGB_IMAGE_TOPIC</code>: topic name for output annotated RGB image, <code class="language-plaintext highlighter-rouge">None</code> to stop the node from publishing on this topic (default=<code class="language-plaintext highlighter-rouge">/opendr/image_objects_annotated</code>)</li>
  <li>
<code class="language-plaintext highlighter-rouge">-d or --detections_topic DETECTIONS_TOPIC</code>: topic name for detection messages, <code class="language-plaintext highlighter-rouge">None</code> to stop the node from publishing on this topic (default=<code class="language-plaintext highlighter-rouge">/opendr/objects</code>)</li>
  <li>
<code class="language-plaintext highlighter-rouge">--performance_topic PERFORMANCE_TOPIC</code>: topic name for performance messages (default=<code class="language-plaintext highlighter-rouge">None</code>, disabled)</li>
  <li>
<code class="language-plaintext highlighter-rouge">--device DEVICE</code>: Device to use, either <code class="language-plaintext highlighter-rouge">cpu</code> or <code class="language-plaintext highlighter-rouge">cuda</code>, falls back to <code class="language-plaintext highlighter-rouge">cpu</code> if GPU or CUDA is not found (default=<code class="language-plaintext highlighter-rouge">cuda</code>)</li>
</ul>

<ol>
  <li>Default output topics:
    <ul>
      <li>Output images: <code class="language-plaintext highlighter-rouge">/opendr/image_objects_annotated</code>
</li>
      <li>Detection messages: <code class="language-plaintext highlighter-rouge">/opendr/objects</code>
</li>
    </ul>

    <p>For viewing the output, refer to the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#notes">notes above.</a></p>
  </li>
</ol>

<h3 id="2d-single-object-tracking-ros2-node">2D Single Object Tracking ROS2 Node</h3>

<p>You can find the single object tracking 2D ROS2 node python script <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/object_tracking_2d_siamrpn_node.py">here</a> to inspect the code and modify it as you wish to fit your needs.
The node makes use of the toolkit’s <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/object_tracking_2d/siamrpn/siamrpn_learner.py">single object tracking 2D SiamRPN tool</a> whose documentation can be found <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/object-tracking-2d-siamrpn.md">here</a>.</p>

<h4 id="instructions-for-basic-usage-5">Instructions for basic usage:</h4>

<ol>
  <li>
    <p>Start the node responsible for publishing images. If you have a USB camera, then you can use the <code class="language-plaintext highlighter-rouge">usb_cam_node</code> as explained in the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#prerequisites">prerequisites above</a>.</p>
  </li>
  <li>
    <p>You are then ready to start the single object tracking 2D node:</p>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ros2 run opendr_perception object_tracking_2d_siamrpn
    
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The following optional arguments are available:    - `-h or --help`: show a help message and exit    - `-i or --input_rgb_image_topic INPUT_RGB_IMAGE_TOPIC` : listen to RGB images on this topic (default=`/image_raw`)    - `-o or --output_rgb_image_topic OUTPUT_RGB_IMAGE_TOPIC`: topic name for output annotated RGB image, `None` to stop the node from publishing on this topic (default=`/opendr/image_tracking_annotated`)    - `-t or --tracker_topic TRACKER_TOPIC`: topic name for tracker messages, `None` to stop the node from publishing on this topic (default=`/opendr/tracked_object`)    - `--performance_topic PERFORMANCE_TOPIC`: topic name for performance messages (default=`None`, disabled)    - `--device DEVICE`: Device to use, either `cpu` or `cuda`, falls back to `cpu` if GPU or CUDA is not found (default=`cuda`)
</code></pre></div></div>

<ol>
  <li>Default output topics:
    <ul>
      <li>Output images: <code class="language-plaintext highlighter-rouge">/opendr/image_tracking_annotated</code>
</li>
      <li>Detection messages: <code class="language-plaintext highlighter-rouge">/opendr/tracked_object</code>
</li>
    </ul>

    <p>For viewing the output, refer to the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#notes">notes above.</a></p>
  </li>
</ol>

<p><strong>Notes</strong></p>

<p>To initialize this node it is required to provide a bounding box of an object to track.
This is achieved by initializing one of the toolkit’s 2D object detectors (YOLOv3) and running object detection once on the input.
Afterwards, <strong>the detected bounding box that is closest to the center of the image</strong> is used to initialize the tracker. 
Feel free to modify the node to initialize it in a different way that matches your use case.</p>

<h3 id="2d-object-tracking-ros2-nodes">2D Object Tracking ROS2 Nodes</h3>

<p>For 2D object tracking, there two ROS2 nodes provided, one using Deep Sort and one using FairMOT which use either pretrained models, or custom trained models.
The predicted tracking annotations are split into two topics with detections and tracking IDs. Additionally, an annotated image is generated.</p>

<p>You can find the 2D object detection ROS2 node python scripts here: <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/object_tracking_2d_deep_sort_node.py">Deep Sort node</a> and <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/object_tracking_2d_fair_mot_node.py">FairMOT node</a>
where you can inspect the code and modify it as you wish to fit your needs.
The nodes makes use of the toolkit’s <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/object_tracking_2d/deep_sort/object_tracking_2d_deep_sort_learner.py">object tracking 2D - Deep Sort tool</a>
and <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/object_tracking_2d/fair_mot/object_tracking_2d_fair_mot_learner.py">object tracking 2D - FairMOT tool</a>
whose documentation can be found here: <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/object-tracking-2d-deep-sort.md">Deep Sort docs</a>, <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/object-tracking-2d-fair-mot.md">FairMOT docs</a>.</p>

<h4 id="instructions-for-basic-usage-6">Instructions for basic usage:</h4>

<ol>
  <li>
    <p>Start the node responsible for publishing images. If you have a USB camera, then you can use the <code class="language-plaintext highlighter-rouge">usb_cam_node</code> as explained in the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#prerequisites">prerequisites above</a>.</p>
  </li>
  <li>
    <p>You are then ready to start a 2D object tracking node:</p>
    <ol>
      <li>Deep Sort node</li>
    </ol>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      ros2 run opendr_perception object_tracking_2d_deep_sort
      
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  The following optional argument is available for the Deep Sort node:
  - `-n --model_name MODEL_NAME`: name of the trained model (default=`deep_sort`)    2. FairMOT node
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      ros2 run opendr_perception object_tracking_2d_fair_mot
      
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  The following optional argument is available for the FairMOT node:
  - `-n --model_name MODEL_NAME`: name of the trained model (default=`fairmot_dla34`)

The following optional arguments are available for both nodes:    - `-h or --help`: show a help message and exit    - `-i or --input_rgb_image_topic INPUT_RGB_IMAGE_TOPIC`: topic name for input RGB image (default=`/image_raw`)    - `-o or --output_rgb_image_topic OUTPUT_RGB_IMAGE_TOPIC`: topic name for output annotated RGB image, `None` to stop the node from publishing on this topic (default=`/opendr/image_objects_annotated`)    - `-d or --detections_topic DETECTIONS_TOPIC`: topic name for detection messages, `None` to stop the node from publishing on this topic (default=`/opendr/objects`)    - `-t or --tracking_id_topic TRACKING_ID_TOPIC`: topic name for tracking ID messages, `None` to stop the node from publishing on this topic (default=`/opendr/objects_tracking_id`)    - `--performance_topic PERFORMANCE_TOPIC`: topic name for performance messages (default=`None`, disabled)    - `--device DEVICE`: device to use, either `cpu` or `cuda`, falls back to `cpu` if GPU or CUDA is not found (default=`cuda`)    - `-td --temp_dir TEMP_DIR`: path to a temporary directory with models (default=`temp`)
</code></pre></div></div>

<ol>
  <li>Default output topics:
    <ul>
      <li>Output images: <code class="language-plaintext highlighter-rouge">/opendr/image_objects_annotated</code>
</li>
      <li>Detection messages: <code class="language-plaintext highlighter-rouge">/opendr/objects</code>
</li>
      <li>Tracking ID messages: <code class="language-plaintext highlighter-rouge">/opendr/objects_tracking_id</code>
</li>
    </ul>

    <p>For viewing the output, refer to the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#notes">notes above.</a></p>
  </li>
</ol>

<p><strong>Notes</strong></p>

<p>An <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#image-dataset-ros2-node">image dataset node</a> is also provided to be used along these nodes.
Make sure to change the default input topic of the tracking node if you are not using the USB cam node.</p>

<h3 id="vision-based-panoptic-segmentation-ros2-node">Vision Based Panoptic Segmentation ROS2 Node</h3>

<p>A ROS node for performing panoptic segmentation on a specified RGB image stream using the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/panoptic_segmentation/README.md#efficientps-efficient-panoptic-segmentation">EfficientPS</a> network.</p>

<p>You can find the vision based panoptic segmentation (EfficientPS) ROS node python script <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/panoptic_segmentation_efficient_ps_node.py">here</a> to inspect the code and modify it as you wish to fit your needs.
The node makes use of the toolkit’s <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/panoptic_segmentation/efficient_ps/efficient_ps_learner.py">panoptic segmentation tool</a> whose documentation can be found <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/efficient-ps.md">here</a>
and additional information about EfficientPS <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/panoptic_segmentation/README.md">here</a>.</p>

<h4 id="instructions-for-basic-usage-7">Instructions for basic usage:</h4>

<ol>
  <li>
    <p>Start the node responsible for publishing images. If you have a USB camera, then you can use the <code class="language-plaintext highlighter-rouge">usb_cam_node</code> as explained in the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#prerequisites">prerequisites above</a>.</p>
  </li>
  <li>
    <p>You are then ready to start the panoptic segmentation node:</p>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ros2 run opendr_perception panoptic_segmentation_efficient_ps
    
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The following optional arguments are available:    - `-h, --help`: show a help message and exit    - `-i or --input_rgb_image_topic INPUT_RGB_IMAGE_TOPIC` : listen to RGB images on this topic (default=`/usb_cam/image_raw`)    - `--checkpoint CHECKPOINT` : download pretrained models [cityscapes, kitti] or load from the provided path (default=`cityscapes`)    - `-oh or --output_heatmap_topic OUTPUT_RGB_IMAGE_TOPIC`: publish the semantic and instance maps on this topic as `OUTPUT_HEATMAP_TOPIC/semantic` and `OUTPUT_HEATMAP_TOPIC/instance` (default=`/opendr/panoptic`)    - `-ov or --output_rgb_image_topic OUTPUT_RGB_IMAGE_TOPIC`: publish the panoptic segmentation map as an RGB image on `VISUALIZATION_TOPIC` or a more detailed overview if using the `--detailed_visualization` flag (default=`/opendr/panoptic/rgb_visualization`)    - `--detailed_visualization`: generate a combined overview of the input RGB image and the semantic, instance, and panoptic segmentation maps and publish it on `OUTPUT_RGB_IMAGE_TOPIC` (default=deactivated)    - `--performance_topic PERFORMANCE_TOPIC`: topic name for performance messages (default=`None`, disabled)
</code></pre></div></div>

<ol>
  <li>Default output topics:
    <ul>
      <li>Output images: <code class="language-plaintext highlighter-rouge">/opendr/panoptic/semantic</code>, <code class="language-plaintext highlighter-rouge">/opendr/panoptic/instance</code>, <code class="language-plaintext highlighter-rouge">/opendr/panoptic/rgb_visualization</code>
</li>
      <li>Detection messages: <code class="language-plaintext highlighter-rouge">/opendr/panoptic/semantic</code>, <code class="language-plaintext highlighter-rouge">/opendr/panoptic/instance</code>
</li>
    </ul>

    <p>For viewing the output, refer to the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#notes">notes above.</a></p>
  </li>
</ol>

<h3 id="semantic-segmentation-bisenet-ros2-node">Semantic Segmentation BiSeNet ROS2 Node</h3>

<p>You can find the semantic segmentation BiSeNet ROS2 node python script <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/semantic_segmentation_bisenet_node.py">here</a> to inspect the code and modify it as you wish to fit your needs.
The node makes use of the toolkit’s <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/semantic_segmentation/bisenet/bisenet_learner.py">semantic segmentation BiSeNet tool</a> whose documentation can be found <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/semantic-segmentation.md">here</a>.</p>

<h4 id="instructions-for-basic-usage-8">Instructions for basic usage:</h4>

<ol>
  <li>
    <p>Start the node responsible for publishing images. If you have a USB camera, then you can use the <code class="language-plaintext highlighter-rouge">usb_cam_node</code> as explained in the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#prerequisites">prerequisites above</a>.</p>
  </li>
  <li>
    <p>You are then ready to start the semantic segmentation BiSeNet node:</p>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ros2 run opendr_perception semantic_segmentation_bisenet
    
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The following optional arguments are available:    - `-h or --help`: show a help message and exit    - `-i or --input_rgb_image_topic INPUT_RGB_IMAGE_TOPIC`: topic name for input RGB image (default=`/image_raw`)    - `-o or --output_heatmap_topic OUTPUT_HEATMAP_TOPIC`: topic to which we are publishing the heatmap in the form of a ROS2 image containing class IDs, `None` to stop the node from publishing on this topic (default=`/opendr/heatmap`)    - `-ov or --output_rgb_image_topic OUTPUT_RGB_IMAGE_TOPIC`: topic to which we are publishing the heatmap image blended with the input image and a class legend for visualization purposes, `None` to stop the node from publishing on this topic (default=`/opendr/heatmap_visualization`)    - `--performance_topic PERFORMANCE_TOPIC`: topic name for performance messages (default=`None`, disabled)    - `--device DEVICE`: device to use, either `cpu` or `cuda`, falls back to `cpu` if GPU or CUDA is not found (default=`cuda`)
</code></pre></div></div>

<ol>
  <li>Default output topics:
    <ul>
      <li>Output images: <code class="language-plaintext highlighter-rouge">/opendr/heatmap</code>, <code class="language-plaintext highlighter-rouge">/opendr/heatmap_visualization</code>
</li>
      <li>Detection messages: <code class="language-plaintext highlighter-rouge">/opendr/heatmap</code>
</li>
    </ul>

    <p>For viewing the output, refer to the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#notes">notes above.</a></p>
  </li>
</ol>

<p><strong>Notes</strong></p>

<p>On the table below you can find the detectable classes and their corresponding IDs:</p>

<table>
  <thead>
    <tr>
      <th>Class</th>
      <th>Bicyclist</th>
      <th>Building</th>
      <th>Car</th>
      <th>Column Pole</th>
      <th>Fence</th>
      <th>Pedestrian</th>
      <th>Road</th>
      <th>Sidewalk</th>
      <th>Sign Symbol</th>
      <th>Sky</th>
      <th>Tree</th>
      <th>Unknown</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>ID</strong></td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>3</td>
      <td>4</td>
      <td>5</td>
      <td>6</td>
      <td>7</td>
      <td>8</td>
      <td>9</td>
      <td>10</td>
      <td>11</td>
    </tr>
  </tbody>
</table>

<h3 id="semantic-segmentation-yolov8-ros2-node">Semantic Segmentation YOLOv8 ROS2 Node</h3>

<p>You can find the semantic segmentation YOLOv8 ROS2 node python script <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/semantic_segmentation_yolov8_node.py">here</a> to inspect the code and modify it as you wish to fit your needs.
The node makes use of the toolkit’s <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/semantic_segmentation/yolov8_seg/yolov8_seg_learner.py">semantic segmentation YOLOv8 tool</a> whose documentation can be found <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/yolov8-seg.md">here</a>.</p>

<p>This node can perform both object detection 2D and semantic segmentation of the objects detected within the bounding boxes.</p>

<h4 id="instructions-for-basic-usage-9">Instructions for basic usage:</h4>

<ol>
  <li>
    <p>Start the node responsible for publishing images. If you have a USB camera, then you can use the <code class="language-plaintext highlighter-rouge">usb_cam_node</code> as explained in the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#prerequisites">prerequisites above</a>.</p>
  </li>
  <li>
    <p>You are then ready to start the semantic segmentation YOLOv8 node:</p>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ros2 run opendr_perception semantic_segmentation_yolov8
    
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The following optional arguments are available:    - `-h or --help`: show a help message and exit    - `-i or --input_rgb_image_topic INPUT_RGB_IMAGE_TOPIC`: topic name for input RGB image (default=`/image_raw`)    - `-o or --output_heatmap_topic OUTPUT_HEATMAP_TOPIC`: topic to which we are publishing the heatmap in the form of a ROS2 image containing class IDs, `None` to stop the node from publishing on this topic (default=`/opendr/heatmap`)    - `-ov or --output_rgb_image_topic OUTPUT_RGB_IMAGE_TOPIC`: topic to which we are publishing the heatmap image blended with the input image visualization purposes, `None` to stop the node from publishing on this topic (default=`/opendr/heatmap_visualization`)    - `-d or --detections_topic DETECTIONS_TOPIC`: topic name for object detection/bounding box messages, `None` to stop the node from publishing on this topic (default=`/opendr/objects`)    - `--performance_topic PERFORMANCE_TOPIC`: topic name for performance messages (default=`None`, disabled)    - `--device DEVICE`: device to use, either `cpu` or `cuda`, falls back to `cpu` if GPU or CUDA is not found (default=`cuda`)    - `--model_name MODEL_NAME`: Network architecture, can be one of `yolov8n-seg`, `yolov8s-seg`, `yolov8m-seg`, `yolov8l-seg`, `yolov8n-segx`, `custom` (default=`yolov8s-seg`)
</code></pre></div></div>

<ol>
  <li>Default output topics:
    <ul>
      <li>Output images: <code class="language-plaintext highlighter-rouge">/opendr/heatmap</code>, <code class="language-plaintext highlighter-rouge">/opendr/heatmap_visualization</code>
</li>
      <li>Detection messages: <code class="language-plaintext highlighter-rouge">/opendr/heatmap</code>, <code class="language-plaintext highlighter-rouge">/opendr/objects</code>
</li>
    </ul>

    <p>For viewing the output, refer to the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#notes">notes above.</a></p>
  </li>
</ol>

<p><strong>Notes</strong></p>

<p>The detected classes can be found 
<a href="https://github.com/ultralytics/ultralytics/blob/9aaa5d5ed0e5a0c1f053069dd73f12b845c4f282/ultralytics/cfg/datasets/coco.yaml#L17">here</a>.</p>

<h3 id="binary-high-resolution-ros2-node">Binary High Resolution ROS2 Node</h3>

<p>You can find the binary high resolution ROS2 node python script <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/binary_high_resolution_node.py">here</a> to inspect the code and modify it as you wish to fit your needs.
The node makes use of the toolkit’s <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/binary_high_resolution/binary_high_resolution_learner.py">binary high resolution tool</a> whose documentation can be found <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/binary_high_resolution.md">here</a>.</p>

<h4 id="instructions-for-basic-usage-10">Instructions for basic usage:</h4>

<ol>
  <li>
    <p>Before running this node it is required to train a model for a specific binary classification task. 
Refer to the tool’s <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/binary_high_resolution.md">documentation</a> for more information.
To test the node out, run <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../python/perception/binary_high_resolution/train_eval_demo.py">train_eval_demo.py</a>
to download the test dataset provided and to train a test model. 
You would then need to move the model folder in <code class="language-plaintext highlighter-rouge">opendr_ws_2</code> so the node can load it using the default <code class="language-plaintext highlighter-rouge">model_path</code> argument.</p>
  </li>
  <li>
    <p>Start the node responsible for publishing images. If you have a USB camera, then you can use the <code class="language-plaintext highlighter-rouge">usb_cam_node</code> as explained in the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#prerequisites">prerequisites above</a>.</p>
  </li>
  <li>
    <p>You are then ready to start the binary high resolution node:</p>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ros2 run opendr_perception binary_high_resolution
    
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The following optional arguments are available:    - `-h or --help`: show a help message and exit    - `-i or --input_rgb_image_topic INPUT_RGB_IMAGE_TOPIC`: topic name for input RGB image (default=`/image_raw`)    - `-o or --output_heatmap_topic OUTPUT_HEATMAP_TOPIC`: topic to which we are publishing the heatmap in the form of a ROS2 image containing class IDs, `None` to stop the node from publishing on this topic (default=`/opendr/binary_hr_heatmap`)    - `-ov or --output_rgb_image_topic OUTPUT_RGB_IMAGE_TOPIC`: topic to which we are publishing the heatmap image blended with the input image and a class legend for visualization purposes, `None` to stop the node from publishing on this topic (default=`/opendr/binary_hr_heatmap_visualization`)    - `-m or --model_path MODEL_PATH`: path to the directory of the trained model (default=`test_model`)    - `-a or --architecture ARCHITECTURE`: architecture used for the trained model, either `VGG_720p` or `VGG_1080p` (default=`VGG_720p`)    - `--performance_topic PERFORMANCE_TOPIC`: topic name for performance messages (default=`None`, disabled)    - `--device DEVICE`: device to use, either `cpu` or `cuda`, falls back to `cpu` if GPU or CUDA is not found (default=`cuda`)
</code></pre></div></div>

<ol>
  <li>Default output topics:
    <ul>
      <li>Output images: <code class="language-plaintext highlighter-rouge">/opendr/binary_hr_heatmap</code>, <code class="language-plaintext highlighter-rouge">/opendr/binary_hr_heatmap_visualization</code>
</li>
      <li>Detection messages: <code class="language-plaintext highlighter-rouge">/opendr/binary_hr_heatmap</code>
</li>
    </ul>

    <p>For viewing the output, refer to the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#notes">notes above.</a></p>
  </li>
</ol>

<h3 id="image-based-facial-emotion-estimation-ros2-node">Image-based Facial Emotion Estimation ROS2 Node</h3>

<p>You can find the image-based facial emotion estimation ROS2 node python script <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/facial_emotion_estimation_node.py">here</a> to inspect the code and modify it as you wish to fit your needs.
The node makes use of the toolkit’s image-based facial emotion estimation tool which can be found <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/facial_expression_recognition/image_based_facial_emotion_estimation/facial_emotion_learner.py">here</a>
whose documentation can be found <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/image_based_facial_emotion_estimation.md">here</a>.</p>

<h4 id="instructions-for-basic-usage-11">Instructions for basic usage:</h4>

<ol>
  <li>
    <p>Start the node responsible for publishing images. If you have a USB camera, then you can use the <code class="language-plaintext highlighter-rouge">usb_cam_node</code> as explained in the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#prerequisites">prerequisites above</a>.</p>
  </li>
  <li>
    <p>You are then ready to start the image-based facial emotion estimation node:</p>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ros2 run opendr_perception facial_emotion_estimation
    
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The following optional arguments are available:    - `-h or --help`: show a help message and exit    - `-i or --input_rgb_image_topic INPUT_RGB_IMAGE_TOPIC`: topic name for input RGB image (default=`/image_raw`)    - `-o or --output_rgb_image_topic OUTPUT_RGB_IMAGE_TOPIC`: topic name for output annotated RGB image, `None` to stop the node from publishing on this topic (default=`/opendr/image_emotion_estimation_annotated`)    - `-e or --output_emotions_topic OUTPUT_EMOTIONS_TOPIC`: topic to which we are publishing the facial emotion results, `None` to stop the node from publishing on this topic (default=`"/opendr/facial_emotion_estimation"`)    - `-m or --output_emotions_description_topic OUTPUT_EMOTIONS_DESCRIPTION_TOPIC`: topic to which we are publishing the description of the estimated facial emotion, `None` to stop the node from publishing on this topic (default=`/opendr/facial_emotion_estimation_description`)    - `--performance_topic PERFORMANCE_TOPIC`: topic name for performance messages (default=`None`, disabled)    - `--device DEVICE`: device to use, either `cpu` or `cuda`, falls back to `cpu` if GPU or CUDA is not found (default=`cuda`)
</code></pre></div></div>

<ol>
  <li>Default output topics:
    <ul>
      <li>Output images: <code class="language-plaintext highlighter-rouge">/opendr/image_emotion_estimation_annotated</code>
</li>
      <li>Detection messages: <code class="language-plaintext highlighter-rouge">/opendr/facial_emotion_estimation</code>, <code class="language-plaintext highlighter-rouge">/opendr/facial_emotion_estimation_description</code>
</li>
    </ul>

    <p>For viewing the output, refer to the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#notes">notes above.</a></p>
  </li>
</ol>

<p><strong>Notes</strong></p>

<p>This node requires the detection of a face first. This is achieved by including of the toolkit’s face detector and running face detection on the input.
Afterwards, the detected bounding box of the face is cropped and fed into the facial emotion estimator. 
Feel free to modify the node to detect faces in a different way that matches your use case.</p>

<h3 id="landmark-based-facial-expression-recognition-ros2-node">Landmark-based Facial Expression Recognition ROS2 Node</h3>

<p>A ROS2 node for performing landmark-based facial expression recognition using a trained model on AFEW, CK+ or Oulu-CASIA datasets.
OpenDR does not include a pretrained model, so one should be provided by the user.
An alternative would be to use the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#image-based-facial-emotion-estimation-ros2-node">image-based facial expression estimation node</a> provided by the toolkit.</p>

<p>You can find the landmark-based facial expression recognition ROS2 node python script <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/landmark_based_facial_expression_recognition_node.py">here</a> to inspect the code and modify it as you wish to fit your needs.
The node makes use of the toolkit’s landmark-based facial expression recognition tool which can be found <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/facial_expression_recognition/landmark_based_facial_expression_recognition/progressive_spatio_temporal_bln_learner.py">here</a>
whose documentation can be found <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/landmark-based-facial-expression-recognition.md">here</a>.</p>

<h4 id="instructions-for-basic-usage-12">Instructions for basic usage:</h4>

<ol>
  <li>
    <p>Start the node responsible for publishing images. If you have a USB camera, then you can use the <code class="language-plaintext highlighter-rouge">usb_cam_node</code> as explained in the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#prerequisites">prerequisites above</a>.</p>
  </li>
  <li>
    <p>You are then ready to start the landmark-based facial expression recognition node:</p>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ros2 run opendr_perception landmark_based_facial_expression_recognition
    
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The following optional arguments are available:    - `-h or --help`: show a help message and exit    - `-i or --input_rgb_image_topic INPUT_RGB_IMAGE_TOPIC`: topic name for input RGB image (default=`/image_raw`)    - `-o or --output_category_topic OUTPUT_CATEGORY_TOPIC`: topic to which we are publishing the recognized facial expression category info, `None` to stop the node from publishing on this topic (default=`"/opendr/landmark_expression_recognition"`)    - `-d or --output_category_description_topic OUTPUT_CATEGORY_DESCRIPTION_TOPIC`: topic to which we are publishing the description of the recognized facial expression, `None` to stop the node from publishing on this topic (default=`/opendr/landmark_expression_recognition_description`)    - `--performance_topic PERFORMANCE_TOPIC`: topic name for performance messages (default=`None`, disabled)    - `--device DEVICE`: device to use, either `cpu` or `cuda`, falls back to `cpu` if GPU or CUDA is not found (default=`cuda`)    - `--model`: architecture to use for facial expression recognition, options are `pstbln_ck+`, `pstbln_casia`, `pstbln_afew` (default=`pstbln_afew`)    - `-s or --shape_predictor SHAPE_PREDICTOR`: shape predictor (landmark_extractor) to use (default=`./predictor_path`)
</code></pre></div></div>

<ol>
  <li>Default output topics:
    <ul>
      <li>Detection messages: <code class="language-plaintext highlighter-rouge">/opendr/landmark_expression_recognition</code>, <code class="language-plaintext highlighter-rouge">/opendr/landmark_expression_recognition_description</code>
</li>
    </ul>

    <p>For viewing the output, refer to the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#notes">notes above.</a></p>
  </li>
</ol>

<h3 id="skeleton-based-human-action-recognition-ros2-nodes">Skeleton-based Human Action Recognition ROS2 Nodes</h3>

<p>A ROS2 node for performing skeleton-based human action recognition is provided, one using either ST-GCN or PST-GCN models pretrained on NTU-RGBD-60 dataset.
Another ROS2 node for performing continual skeleton-based human action recognition is provided, using the CoSTGCN method. 
The human body poses of the image are first extracted by the lightweight OpenPose method which is implemented in the toolkit, and they are passed to the skeleton-based action recognition methods to be categorized.</p>

<p>You can find the skeleton-based human action recognition ROS2 node python script <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/skeleton_based_action_recognition_node.py">here</a> 
and the continual skeleton-based human action recognition ROS2 node python script <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/continual_skeleton_based_action_recognition_node.py">here</a> to inspect the code and modify it as you wish to fit your needs.
The latter makes use of the toolkit’s skeleton-based human action recognition tool which can be found <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/skeleton_based_action_recognition/spatio_temporal_gcn_learner.py">here for ST-GCN</a>
and <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/skeleton_based_action_recognition/progressive_spatio_temporal_gcn_learner.py">here for PST-GCN</a> and the former makes use
of the toolkit’s continual skeleton-based human action recognition tool which can be found <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/skeleton_based_action_recognition/continual_stgcn_learner.py">here</a>.
Their documentation can be found <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/skeleton-based-action-recognition.md">here</a>.</p>

<h4 id="instructions-for-basic-usage-13">Instructions for basic usage:</h4>

<ol>
  <li>
    <p>Start the node responsible for publishing images. If you have a USB camera, then you can use the <code class="language-plaintext highlighter-rouge">usb_cam_node</code> as explained in the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#prerequisites">prerequisites above</a>.</p>
  </li>
  <li>
    <p>You are then ready to start the skeleton-based human action recognition node:</p>
    <ol>
      <li>Skeleton-based action recognition node</li>
    </ol>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      ros2 run opendr_perception skeleton_based_action_recognition
      
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  The following optional argument is available for the skeleton-based action recognition node:
  - `--model` MODEL: model to use, options are `stgcn` or `pstgcn`, (default=`stgcn`)
  - `-c or --output_category_topic OUTPUT_CATEGORY_TOPIC`: topic name for recognized action category, `None` to stop the node from publishing on this topic (default=`"/opendr/skeleton_recognized_action"`)
  - `-d or --output_category_description_topic OUTPUT_CATEGORY_DESCRIPTION_TOPIC`: topic name for description of the recognized action category, `None` to stop the node from publishing on this topic (default=`/opendr/skeleton_recognized_action_description`)
</code></pre></div></div>

<ol>
  <li>Continual skeleton-based action recognition node</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      ros2 run opendr_perception continual_skeleton_based_action_recognition
      
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  The following optional argument is available for the continual skeleton-based action recognition node:
  - `--model` MODEL: model to use, options are `costgcn`, (default=`costgcn`)
  - `-c or --output_category_topic OUTPUT_CATEGORY_TOPIC`: topic name for recognized action category, `None` to stop the node from publishing on this topic (default=`"/opendr/continual_skeleton_recognized_action"`)
  - `-d or --output_category_description_topic OUTPUT_CATEGORY_DESCRIPTION_TOPIC`: topic name for description of the recognized action category, `None` to stop the node from publishing on this topic (default=`/opendr/continual_skeleton_recognized_action_description`)

The following optional arguments are available for all nodes:    - `-h or --help`: show a help message and exit    - `-i or --input_rgb_image_topic INPUT_RGB_IMAGE_TOPIC`: topic name for input RGB image (default=`/usb_cam/image_raw`)    - `-o or --output_rgb_image_topic OUTPUT_RGB_IMAGE_TOPIC`: topic name for output pose-annotated RGB image, `None` to stop the node from publishing on this topic (default=`/opendr/image_pose_annotated`)    - `-p or --pose_annotations_topic POSE_ANNOTATIONS_TOPIC`: topic name for pose annotations, `None` to stop the node from publishing on this topic (default=`/opendr/poses`)    - `--performance_topic PERFORMANCE_TOPIC`: topic name for performance messages (default=`None`, disabled)    - `--device DEVICE`: device to use, either `cpu` or `cuda`, falls back to `cpu` if GPU or CUDA is not found (default=`cuda`)
</code></pre></div></div>

<ol>
  <li>Default output topics:
    <ol>
      <li>Skeleton-based action recognition node:
        <ul>
          <li>Detection messages: <code class="language-plaintext highlighter-rouge">/opendr/skeleton_based_action_recognition</code>, <code class="language-plaintext highlighter-rouge">/opendr/skeleton_based_action_recognition_description</code>, <code class="language-plaintext highlighter-rouge">/opendr/poses</code>
</li>
          <li>Output images: <code class="language-plaintext highlighter-rouge">/opendr/image_pose_annotated</code>
</li>
        </ul>
      </li>
      <li>Continual skeleton-based action recognition node:
        <ul>
          <li>Detection messages: <code class="language-plaintext highlighter-rouge">/opendr/continual_skeleton_recognized_action</code>, <code class="language-plaintext highlighter-rouge">/opendr/continual_skeleton_recognized_action_description</code>, <code class="language-plaintext highlighter-rouge">/opendr/poses</code>
</li>
          <li>Output images: <code class="language-plaintext highlighter-rouge">/opendr/image_pose_annotated</code>
</li>
        </ul>

        <p>For viewing the output, refer to the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#notes">notes above.</a></p>
      </li>
    </ol>
  </li>
</ol>

<h3 id="video-human-activity-recognition-ros2-node">Video Human Activity Recognition ROS2 Node</h3>

<p>A ROS2 node for performing human activity recognition using either CoX3D or X3D models pretrained on Kinetics400.</p>

<p>You can find the video human activity recognition ROS2 node python script <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/video_activity_recognition_node.py">here</a> to inspect the code and modify it as you wish to fit your needs.
The node makes use of the toolkit’s video human activity recognition tools which can be found <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/activity_recognition/cox3d/cox3d_learner.py">here for CoX3D</a> and
<a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/activity_recognition/x3d/x3d_learner.py">here for X3D</a> whose documentation can be found <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/activity-recognition.md">here</a>.</p>

<h4 id="instructions-for-basic-usage-14">Instructions for basic usage:</h4>

<ol>
  <li>
    <p>Start the node responsible for publishing images. If you have a USB camera, then you can use the <code class="language-plaintext highlighter-rouge">usb_cam_node</code> as explained in the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#prerequisites">prerequisites above</a>.</p>
  </li>
  <li>
    <p>You are then ready to start the video human activity recognition node:</p>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ros2 run opendr_perception video_activity_recognition
    
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The following optional arguments are available:    - `-h or --help`: show a help message and exit    - `-i or --input_rgb_image_topic INPUT_RGB_IMAGE_TOPIC`: topic name for input RGB image (default=`/image_raw`)    - `-o or --output_category_topic OUTPUT_CATEGORY_TOPIC`: topic to which we are publishing the recognized activity, `None` to stop the node from publishing on this topic (default=`"/opendr/human_activity_recognition"`)    - `-od or --output_category_description_topic OUTPUT_CATEGORY_DESCRIPTION_TOPIC`: topic to which we are publishing the ID of the recognized action, `None` to stop the node from publishing on this topic (default=`/opendr/human_activity_recognition_description`)    - `--performance_topic PERFORMANCE_TOPIC`: topic name for performance messages (default=`None`, disabled)    - `--model`: architecture to use for human activity recognition, options are `cox3d-s`, `cox3d-m`, `cox3d-l`, `x3d-xs`, `x3d-s`, `x3d-m`, or `x3d-l` (default=`cox3d-m`)    - `--device DEVICE`: device to use, either `cpu` or `cuda`, falls back to `cpu` if GPU or CUDA is not found (default=`cuda`)
</code></pre></div></div>

<ol>
  <li>Default output topics:
    <ul>
      <li>Detection messages: <code class="language-plaintext highlighter-rouge">/opendr/human_activity_recognition</code>, <code class="language-plaintext highlighter-rouge">/opendr/human_activity_recognition_description</code>
</li>
    </ul>

    <p>For viewing the output, refer to the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#notes">notes above.</a></p>
  </li>
</ol>

<p><strong>Notes</strong></p>

<p>You can find the corresponding IDs regarding activity recognition <a href="https://github.com/opendr-eu/opendr/blob/master/src/opendr/perception/activity_recognition/datasets/kinetics400_classes.csv">here</a>.</p>

<h3 id="rgb-gesture-recognition-ros2-node">RGB Gesture Recognition ROS2 Node</h3>

<p>For gesture recognition, the ROS2 <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/gesture_recognition_node.py">node</a> is based on the gesture recognition learner defined <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/gesture_recognition/gesture_recognition_learner.py">here</a>, and the documentation of the learner can be found <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/gesture-recognition-learner.md">here</a>.</p>

<h4 id="instructions-for-basic-usage-15">Instructions for basic usage:</h4>

<ol>
  <li>
    <p>Start the node responsible for publishing images. If you have a USB camera, then you can use the <code class="language-plaintext highlighter-rouge">usb_cam_node</code> as explained in the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#prerequisites">prerequisites above</a>.</p>
  </li>
  <li>
    <p>Start the gesture recognition node:</p>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   ros2 run opendr_perception gesture_recognition
   
</code></pre></div></div>
<p>The following arguments are available:</p>
<ul>
  <li>
<code class="language-plaintext highlighter-rouge">-i or --input_rgb_image_topic INPUT_RGB_IMAGE_TOPIC</code>: topic name for input RGB image (default=<code class="language-plaintext highlighter-rouge">/usb_cam/image_raw</code>)</li>
  <li>
<code class="language-plaintext highlighter-rouge">-o or --output_rgb_image_topic OUTPUT_RGB_IMAGE_TOPIC</code>: topic name for output annotated RGB image (default=<code class="language-plaintext highlighter-rouge">/opendr/image_gesture_annotated</code>)</li>
  <li>
<code class="language-plaintext highlighter-rouge">-d or --detections_topic DETECTIONS_TOPIC</code>: topic name for detection messages (default=<code class="language-plaintext highlighter-rouge">/opendr/gestures</code>)</li>
  <li>
<code class="language-plaintext highlighter-rouge">--performance_topic PERFORMANCE_TOPIC</code>: topic name for performance messages (default=<code class="language-plaintext highlighter-rouge">None</code>, disabled)</li>
  <li>
<code class="language-plaintext highlighter-rouge">--device DEVICE</code>: Device to use, either <code class="language-plaintext highlighter-rouge">cpu</code> or <code class="language-plaintext highlighter-rouge">cuda</code>, falls back to <code class="language-plaintext highlighter-rouge">cpu</code> if GPU or CUDA is not found (default=<code class="language-plaintext highlighter-rouge">cuda</code>)</li>
  <li>
<code class="language-plaintext highlighter-rouge">--threshold THRESHOLD</code>: Confidence threshold for predictions (default=0.5)</li>
  <li>
<code class="language-plaintext highlighter-rouge">--model MODEL</code>: Config file name of the model that will be used (default=<code class="language-plaintext highlighter-rouge">plus_m_1.5x_416)</code>
</li>
</ul>

<ol>
  <li>Default output topics:
    <ul>
      <li>Output images: <code class="language-plaintext highlighter-rouge">/opendr/image_gesture_annotated</code>
</li>
      <li>Detection messages: <code class="language-plaintext highlighter-rouge">/opendr/gestures</code>
</li>
    </ul>
  </li>
</ol>

<h2 id="rgb--infrared-input">RGB + Infrared input</h2>

<h3 id="2d-object-detection-gem-ros2-node">2D Object Detection GEM ROS2 Node</h3>

<p>You can find the object detection 2D GEM ROS2 node python script <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/object_detection_2d_gem_node.py">here</a> to inspect the code and modify it as you wish to fit your needs.
The node makes use of the toolkit’s <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/object_detection_2d/gem/gem_learner.py">object detection 2D GEM tool</a>
whose documentation can be found <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/gem.md">here</a>.</p>

<h4 id="instructions-for-basic-usage-16">Instructions for basic usage:</h4>

<ol>
  <li>
    <p>First one needs to find points in the color and infrared images that correspond, in order to find the homography matrix that allows to correct for the difference in perspective between the infrared and the RGB camera.
These points can be selected using a <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/object_detection_2d/utils/get_color_infra_alignment.py">utility tool</a> that is provided in the toolkit.</p>
  </li>
  <li>
    <p>Pass the points you have found as <em>pts_color</em> and <em>pts_infra</em> arguments to the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/object_detection_2d_gem.py">ROS2 GEM node</a>.</p>
  </li>
  <li>
    <p>Start the node responsible for publishing images. If you have a RealSense camera, then you can use the corresponding node (assuming you have installed <a href="http://wiki.ros.org/realsense2_camera">realsense2_camera</a>):</p>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   roslaunch realsense2_camera rs_camera.launch enable_color:<span class="o">=</span><span class="nb">true </span>enable_infra:<span class="o">=</span><span class="nb">true </span>enable_depth:<span class="o">=</span><span class="nb">false </span>enable_sync:<span class="o">=</span><span class="nb">true </span>infra_width:<span class="o">=</span>640 infra_height:<span class="o">=</span>480
   
</code></pre></div></div>

<ol>
  <li>You are then ready to start the object detection 2d GEM node:</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ros2 run opendr_perception object_detection_2d_gem
    
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The following optional arguments are available:    - `-h or --help`: show a help message and exit    - `-ic or --input_rgb_image_topic INPUT_RGB_IMAGE_TOPIC`: topic name for input RGB image (default=`/camera/color/image_raw`)    - `-ii or --input_infra_image_topic INPUT_INFRA_IMAGE_TOPIC`: topic name for input infrared image (default=`/camera/infra/image_raw`)    - `-oc or --output_rgb_image_topic OUTPUT_RGB_IMAGE_TOPIC`: topic name for output annotated RGB image, `None` to stop the node from publishing on this topic (default=`/opendr/rgb_image_objects_annotated`)    - `-oi or --output_infra_image_topic OUTPUT_INFRA_IMAGE_TOPIC`: topic name for output annotated infrared image, `None` to stop the node from publishing on this topic (default=`/opendr/infra_image_objects_annotated`)    - `-d or --detections_topic DETECTIONS_TOPIC`: topic name for detection messages, `None` to stop the node from publishing on this topic (default=`/opendr/objects`)    - `--performance_topic PERFORMANCE_TOPIC`: topic name for performance messages (default=`None`, disabled)    - `--device DEVICE`: device to use, either `cpu` or `cuda`, falls back to `cpu` if GPU or CUDA is not found (default=`cuda`)
</code></pre></div></div>

<ol>
  <li>Default output topics:
    <ul>
      <li>Output RGB images: <code class="language-plaintext highlighter-rouge">/opendr/rgb_image_objects_annotated</code>
</li>
      <li>Output infrared images: <code class="language-plaintext highlighter-rouge">/opendr/infra_image_objects_annotated</code>
</li>
      <li>Detection messages: <code class="language-plaintext highlighter-rouge">/opendr/objects</code>
</li>
    </ul>

    <p>For viewing the output, refer to the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#notes">notes above.</a></p>
  </li>
</ol>

<hr>
<h2 id="rgbd-input">RGBD input</h2>

<h3 id="rgbd-hand-gesture-recognition-ros2-node">RGBD Hand Gesture Recognition ROS2 Node</h3>
<p>A ROS2 node for performing hand gesture recognition using a MobileNetv2 model trained on HANDS dataset.
The node has been tested with Kinectv2 for depth data acquisition with the following drivers: https://github.com/OpenKinect/libfreenect2 and https://github.com/code-iai/iai_kinect2.</p>

<p>You can find the RGBD hand gesture recognition ROS2 node python script <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/rgbd_hand_gesture_recognition_node.py">here</a> to inspect the code and modify it as you wish to fit your needs.
The node makes use of the toolkit’s <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/multimodal_human_centric/rgbd_hand_gesture_learner/rgbd_hand_gesture_learner.py">hand gesture recognition tool</a>
whose documentation can be found <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/rgbd-hand-gesture-learner.md">here</a>.</p>

<h4 id="instructions-for-basic-usage-17">Instructions for basic usage:</h4>

<ol>
  <li>
    <p>Start the node responsible for publishing images from an RGBD camera. Remember to modify the input topics using the arguments in step 2 if needed.</p>
  </li>
  <li>
    <p>You are then ready to start the hand gesture recognition node:</p>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ros2 run opendr_perception rgbd_hand_gesture_recognition
    
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The following optional arguments are available:    - `-h or --help`: show a help message and exit    - `-ic or --input_rgb_image_topic INPUT_RGB_IMAGE_TOPIC`: topic name for input RGB image (default=`/kinect2/qhd/image_color_rect`)    - `-id or --input_depth_image_topic INPUT_DEPTH_IMAGE_TOPIC`: topic name for input depth image (default=`/kinect2/qhd/image_depth_rect`)    - `-o or --output_gestures_topic OUTPUT_GESTURES_TOPIC`: topic name for predicted gesture class (default=`/opendr/gestures`)    - `--performance_topic PERFORMANCE_TOPIC`: topic name for performance messages (default=`None`, disabled)    - `--device DEVICE`: device to use, either `cpu` or `cuda`, falls back to `cpu` if GPU or CUDA is not found (default=`cuda`)
</code></pre></div></div>

<ol>
  <li>Default output topics:
    <ul>
      <li>Detection messages:<code class="language-plaintext highlighter-rouge">/opendr/gestures</code>
</li>
    </ul>

    <p>For viewing the output, refer to the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#notes">notes above.</a></p>
  </li>
</ol>

<hr>
<h2 id="rgb--audio-input">RGB + Audio input</h2>

<h3 id="audiovisual-emotion-recognition-ros2-node">Audiovisual Emotion Recognition ROS2 Node</h3>

<p>You can find the audiovisual emotion recognition ROS2 node python script <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/audiovisual_emotion_recognition_node.py">here</a> to inspect the code and modify it as you wish to fit your needs.
The node makes use of the toolkit’s <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/multimodal_human_centric/audiovisual_emotion_learner/avlearner.py">audiovisual emotion recognition tool</a>,
whose documentation can be found <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/audiovisual-emotion-recognition-learner.md">here</a>.</p>

<h4 id="instructions-for-basic-usage-18">Instructions for basic usage:</h4>

<ol>
  <li>Start the node responsible for publishing images. If you have a USB camera, then you can use the <code class="language-plaintext highlighter-rouge">usb_cam_node</code> as explained in the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#prerequisites">prerequisites above</a>.</li>
  <li>Start the node responsible for publishing audio. If you have an audio capture device, then you can use the <code class="language-plaintext highlighter-rouge">audio_capture_node</code> as explained in the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#prerequisites">prerequisites above</a>.</li>
  <li>You are then ready to start the audiovisual emotion recognition node</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ros2 run opendr_perception audiovisual_emotion_recognition
    
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The following optional arguments are available:    - `-h or --help`: show a help message and exit    - `-iv or --input_video_topic INPUT_VIDEO_TOPIC`: topic name for input video, expects detected face of size 224x224 (default=`/image_raw`)    - `-ia or --input_audio_topic INPUT_AUDIO_TOPIC`: topic name for input audio (default=`/audio`)    - `-o or --output_emotions_topic OUTPUT_EMOTIONS_TOPIC`: topic to which we are publishing the predicted emotion (default=`/opendr/audiovisual_emotion`)    - `--performance_topic PERFORMANCE_TOPIC`: topic name for performance messages (default=`None`, disabled)    - `--buffer_size BUFFER_SIZE`: length of audio and video in seconds, (default=`3.6`)    - `--model_path MODEL_PATH`: if given, the pretrained model will be loaded from the specified local path, otherwise it will be downloaded from an OpenDR FTP server
</code></pre></div></div>

<ol>
  <li>Default output topics:
    <ul>
      <li>Detection messages: <code class="language-plaintext highlighter-rouge">/opendr/audiovisual_emotion</code>
</li>
    </ul>

    <p>For viewing the output, refer to the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#notes">notes above.</a></p>
  </li>
</ol>

<hr>
<h2 id="rgb--imu-input">RGB + IMU input</h2>

<h3 id="continual-slam-ros2-nodes">Continual SLAM ROS2 Nodes</h3>
<p>A ROS node for performing depth+position output mapping based on visual + imu input. Continual SLAM involves the use of two distinct ROS nodes, one dedicated to performing inference and the other exclusively focused on training. Both of the nodes are based on the learner class defined in <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/continual_slam/continual_slam_learner.py">ContinualSLAMLearner</a>.</p>

<p>You can find the continual slam ROS node python scripts here <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/continual_slam_learner_node.py">learner</a>, <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/continual_slam_predictor_node.py">predictor</a>. You can further also find the RGB image + IMU publisher node <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/continual_slam_dataset_node.py">here</a>.</p>

<h4 id="instructions-for-basic-usage-19">Instructions for basic usage:</h4>

<ol>
  <li>
    <p>Download the KITTI Visual Odometry datased as it is described <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/continual_slam/datasets/README.md">here</a>.</p>
  </li>
  <li>
    <p>Decide on the frame rate FPS, then one can start the dataset publisher node using the following line:</p>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   ros2 run opendr_perception continual_slam_dataset
   
</code></pre></div></div>

<p>The following optional arguments are available:</p>
<ul>
  <li>
<code class="language-plaintext highlighter-rouge">-h or --help</code>: show a help message and exit</li>
  <li>
<code class="language-plaintext highlighter-rouge">--dataset_path</code>: path to the dataset (default=<code class="language-plaintext highlighter-rouge">./kitti</code>)</li>
  <li>
<code class="language-plaintext highlighter-rouge">--config_file_path</code>: path to the config file for learner class (default=<code class="language-plaintext highlighter-rouge">src/opendr/perception/continual_slam/configs/singlegpu_kitti.yaml</code>)</li>
  <li>
<code class="language-plaintext highlighter-rouge">--output_image_topic OUTPUT_IMAGE_TOPIC</code>: topic to which we are publishing the RGB image (default=<code class="language-plaintext highlighter-rouge">/cl_slam/image</code>)</li>
  <li>
<code class="language-plaintext highlighter-rouge">--output_distance_topic OUTPUT_DISTANCE_TOPIC</code>: topic to publish distances (default=<code class="language-plaintext highlighter-rouge">/cl_slam/distance</code>)</li>
  <li>
<code class="language-plaintext highlighter-rouge">--dataset_fps FPS</code>: frame rate which the dataset will be published, (default=<code class="language-plaintext highlighter-rouge">3</code>)</li>
</ul>

<ol>
  <li>Start the Predictor Node</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ros2 run opendr_perception continual_slam_predictor
    
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The following optional arguments are available:    - `-h or --help`: show a help message and exit    - `-c or --config_path`: path to the config file for the learner class (default=`src/opendr/perception/continual_slam/configs/singlegpu_kitti.yaml`)    - `-it or --input_image_topic`: input image topic, listened from Continual SLAM Dataset Node (default=`/cl_slam/image`)    - `-dt or --input_distance_topic`: input distance topic, listened from Continual SLAM Dataset Node (default=`/cl_slam/distance`)    - `-odt or --output_depth_topic`: output depth topic, published to visual output tools (default=`/opendr/predicted/image`)    - `-opt or --output_pose_topic`: output pose topic, published to visual output tools (default=`/opendr/predicted/pose`)    - `-ppcl or --publish_pointcloud`: boolean to decide whether pointcloud output is asked or not (default=`false`)    - `-opct or --output_pointcloud_topic`: output pointcloud topic, depending on `--publish_pointcloud`, published to visual output tools (default=`/opendr/predicted/pointcloud`)    - `-ut or --update_topic`: update topic, listened from Continual SLAM Dataset Node (default=`/cl_slam/update`)
</code></pre></div></div>

<ol>
  <li>Start the Learner Node (Optional)</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ros2 run opendr_perception continual_slam_learner
    
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The following optional arguments are available:    - `-h or --help`: show a help message and exit    - `-c or --config_path`: path to the config file for the learner class (default=`src/opendr/perception/continual_slam/configs/singlegpu_kitti.yaml`)    - `-it or --input_image_topic`: input image topic, listened from Continual SLAM Dataset Node (default=`/cl_slam/image`)    - `-dt or --input_distance_topic`: input distance topic, listened from Continual SLAM Dataset Node (default=`/cl_slam/distance`)    - `-ot or --output_weights_topic`: output weights topic to be published to Continual SLAM Predictor Node (default=`/cl_slam/update`)    - `-pr or --publish_rate`: publish rate of the weights (default=`20`)    - `-bs or --buffer_size`: size of the replay buffer (default=`10`)    - `-ss or --sample_size`: sample size of the replay buffer. If 0 is given, only online data is used (default=`3`)    - `-sm or --save_memory`: whether to save memory or not. Add it to the command if you want to write to disk (default=`True`)
</code></pre></div></div>

<hr>
<h2 id="audio-input">Audio input</h2>

<h3 id="speech-command-recognition-ros2-node">Speech Command Recognition ROS2 Node</h3>

<p>A ROS2 node for recognizing speech commands from an audio stream using MatchboxNet, EdgeSpeechNets or Quadratic SelfONN models, pretrained on the Google Speech Commands dataset.</p>

<p>You can find the speech command recognition ROS2 node python script <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/speech_command_recognition_node.py">here</a> to inspect the code and modify it as you wish to fit your needs.
The node makes use of the toolkit’s speech command recognition tools:
<a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/speech_recognition/edgespeechnets/edgespeechnets_learner.py">EdgeSpeechNets tool</a>, <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/speech_recognition/matchboxnet/matchboxnet_learner.py">MatchboxNet tool</a>, <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/speech_recognition/quadraticselfonn/quadraticselfonn_learner.py">Quadratic SelfONN tool</a>
whose documentation can be found here:
<a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/edgespeechnets.md">EdgeSpeechNet docs</a>, <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/matchboxnet.md">MatchboxNet docs</a>, <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/quadratic-selfonn.md">Quadratic SelfONN docs</a>.</p>

<h4 id="instructions-for-basic-usage-20">Instructions for basic usage:</h4>

<ol>
  <li>
    <p>Start the node responsible for publishing audio. If you have an audio capture device, then you can use the <code class="language-plaintext highlighter-rouge">audio_capture_node</code> as explained in the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#prerequisites">prerequisites above</a>.</p>
  </li>
  <li>
    <p>You are then ready to start the speech command recognition node</p>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ros2 run opendr_perception speech_command_recognition
    
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The following optional arguments are available:    - `-h or --help`: show a help message and exit    - `-i or --input_audio_topic INPUT_AUDIO_TOPIC`: topic name for input audio (default=`/audio`)    - `-o or --output_speech_command_topic OUTPUT_SPEECH_COMMAND_TOPIC`: topic name for speech command output (default=`/opendr/speech_recognition`)    - `--performance_topic PERFORMANCE_TOPIC`: topic name for performance messages (default=`None`, disabled)    - `--buffer_size BUFFER_SIZE`: set the size of the audio buffer (expected command duration) in seconds (default=`1.5`)    - `--model MODEL`: the model to use, choices are `matchboxnet`, `edgespeechnets` or `quad_selfonn` (default=`matchboxnet`)    - `--model_path MODEL_PATH`: if given, the pretrained model will be loaded from the specified local path, otherwise it will be downloaded from an OpenDR FTP server
</code></pre></div></div>

<ol>
  <li>Default output topics:
    <ul>
      <li>Detection messages, class id and confidence: <code class="language-plaintext highlighter-rouge">/opendr/speech_recognition</code>
</li>
    </ul>

    <p>For viewing the output, refer to the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#notes">notes above.</a></p>
  </li>
</ol>

<p><strong>Notes</strong></p>

<p>EdgeSpeechNets currently does not have a pretrained model available for download, only local files may be used.</p>

<h3 id="speech-transcription-ros2-node">Speech Transcription ROS2 Node</h3>

<p>A ROS2 node for speech transcription from an audio stream using Whisper or Vosk.</p>

<p>You can find the speech transcription ROS node python script <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/speech_transcription_node.py">here</a> to inspect the code and modify it as you wish to fit your needs.</p>

<p>The node makes use of the toolkit’s speech transcription tools:
<a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/speech_transcription/whisper/whisper_learner.py">Whipser tool</a>, <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/speech_transcription/vosk/vosk_learner.py">Vosk tool</a> whose documentation can be found here:
<a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/speech-transcription-whisper.md">Whisper docs</a>, <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/speech-transcription-vosk.md">Vosk docs</a>.</p>

<h4 id="instruction-for-basic-usage">Instruction for basic usage:</h4>

<ol>
  <li>Start the node responsible for publishing audio. The ROS2 node only work with audio data in WAVE format. If you have an audio capture device, then you can use the <code class="language-plaintext highlighter-rouge">audio_capture_node</code> as explained in the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#prerequisites">prerequisites above</a>.</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ros2 launch audio_capture capture_wave.launch.xml
    
</code></pre></div></div>

<ol>
  <li>You are then ready to start the speech transcription node</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c"># Enable log to console.</span>
    ros2 run opendr_perception speech_transcription <span class="nt">--verbose</span> True
    
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c"># Use Whisper instead of Vosk and choose tiny.en variant.</span>
    ros2 run opendr_perception speech_transcription <span class="nt">--backbone</span> whisper <span class="nt">--model_name</span> tiny.en <span class="nt">--verbose</span> True
    
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c"># Suggest to Whisper that the speech will contain the name 'Felix'.</span>
    ros2 run opendr_perception speech_transcription <span class="nt">--backbone</span> whisper <span class="nt">--model_name</span> tiny.en <span class="nt">--initial_prompt</span> <span class="s2">"Felix"</span> <span class="nt">--verbose</span> True
    
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The following optional arguments are available (More in the source code):    - `-h or --help`: show a help message and exit    - `-i or --input_audio_topic INPUT_AUDIO_TOPIC`: topic name for input audio (default=`/audio/audio`)    - `-o or --output_speech_transcription_topic OUTPUT_TRANSCRIPTION_TOPIC`: topic name for speech transcription output (default=`/opendr/speech_transcription`)    - `--performance_topic PERFORMANCE_TOPIC`: topic name for performance messages (default=`None`, disabled)    - `--backbone {vosk,whisper}`: Backbone model for speech transcription    - `--model_name MODEL_NAME`: Specific model name for each backbone. Example: 'tiny', 'tiny.en', 'base', 'base.en' for Whisper, 'vosk-model-small-en-us-0.15' for Vosk (default=`None`)     - `--model_path MODEL_PATH`: Path to downloaded model files (default=`None`)     - `--language LANGUAGE`: Whisper uses the language parameter to avoid language dectection. Vosk uses the langauge paremeter to select a specific model. Example: 'en' for Whisper, 'en-us' for Vosk (default=`en-us`). Check the available language codes for Whisper at [Whipser repository](https://github.com/openai/whisper/blob/e8622f9afc4eba139bf796c210f5c01081000472/whisper/tokenizer.py#L10). Check the available language code for Vosk from the Vosk model name at [Vosk website](https://alphacephei.com/vosk/models).    - `--initial_prompt INITIAL_PROMPT`: Prompt to provide some context or instruction for the transcription, only for Whisper    - `--verbose VERBOSE`: Display transcription (default=`False`) 
</code></pre></div></div>

<ol>
  <li>Default output topics:
    <ul>
      <li>Speech transcription: <code class="language-plaintext highlighter-rouge">/opendr/speech_transcription</code>
</li>
    </ul>

    <p>For viewing the output, refer to the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#notes">notes above.</a></p>
  </li>
</ol>

<hr>
<h2 id="text-input">Text input</h2>

<h3 id="intent-recognition-ros2-node">Intent Recognition ROS2 Node</h3>

<p>A ROS2 node for recognizing intents from language.
This node should be used together with the speech transcription node that would transcribe the speech into text and infer intent from it.
The provided intent recognition node subscribes to the speech transcription output topic.</p>

<p>You can find the intent recognition ROS node python script <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/intent_recognition_node.py">here</a> to inspect the code and modify if you wish for your needs.
The node makes use of the toolkit’s intent recognition <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/multimodal_human_centric/intent_recognition_learner/intent_recognition_learner.py">learner</a>, and the documentation can be found <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/intent-recognition-learner.md">here</a>.</p>

<h4 id="instructions-for-basic-usage-21">Instructions for basic usage:</h4>

<ol>
  <li>
    <p>Follow the instructions of the speech transcription node and start it.</p>
  </li>
  <li>
    <p>Start the intent recognition node</p>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ros2 run opendr_perception intent_recognition
    
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The following arguments are available:    - `-i or --input_transcription_topic INPUT_TRANSCRIPTION_TOPIC`: topic name for input transcription of type OpenDRTranscription (default=`/opendr/speech_transcription`)    - `-o or --output_intent_topic OUTPUT_INTENT_TOPIC`: topic name for predicted intent (default=`/opendr/intent`)    - `--performance_topic PERFORMANCE_TOPIC`: topic name for performance messages (default=`None`, disabled)    - `--device DEVICE`: device to be used for inference (default=`cuda`)    - `--text_backbone TEXT_BACKBONE`: text backbone tobe used, choices are `bert-base-uncased`, `albert-base-v2`, `bert-small`, `bert-mini`, `bert-tiny` (default=`bert-base-uncased`)    - `--cache_path CACHE_PATH`: cache path for tokenizer files (default=`./cache/`)
</code></pre></div></div>

<ol>
  <li>Default output topics:
    <ul>
      <li>Predicted intents and confidence: <code class="language-plaintext highlighter-rouge">/opendr/intent</code>
</li>
    </ul>

    <p>For viewing the output, refer to the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#notes">notes above.</a></p>
  </li>
</ol>

<p><strong>Notes</strong></p>

<p>On the table below you can find the detectable classes and their corresponding IDs:</p>

<table>
  <thead>
    <tr>
      <th>Class</th>
      <th>Complain</th>
      <th>Praise</th>
      <th>Apologise</th>
      <th>Thank</th>
      <th>Criticize</th>
      <th>Agree</th>
      <th>Taunt</th>
      <th>Flaunt</th>
      <th>Joke</th>
      <th>Oppose</th>
      <th>Comfort</th>
      <th>Care</th>
      <th>Inform</th>
      <th>Advise</th>
      <th>Arrange</th>
      <th>Introduce</th>
      <th>Leave</th>
      <th>Prevent</th>
      <th>Greet</th>
      <th>Ask for help</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>ID</strong></td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>3</td>
      <td>4</td>
      <td>5</td>
      <td>6</td>
      <td>7</td>
      <td>8</td>
      <td>9</td>
      <td>10</td>
      <td>11</td>
      <td>12</td>
      <td>13</td>
      <td>14</td>
      <td>15</td>
      <td>16</td>
      <td>17</td>
      <td>18</td>
      <td>19</td>
    </tr>
  </tbody>
</table>

<hr>
<h2 id="point-cloud-input">Point cloud input</h2>

<h3 id="3d-object-detection-voxel-ros2-node">3D Object Detection Voxel ROS2 Node</h3>

<p>A ROS2 node for performing 3D object detection Voxel using PointPillars or TANet methods with either pretrained models on KITTI dataset, or custom trained models.</p>

<p>You can find the 3D object detection Voxel ROS2 node python script <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/object_detection_3d_voxel_node.py">here</a> to inspect the code and modify it as you wish to fit your needs.
The node makes use of the toolkit’s <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/object_detection_3d/voxel_object_detection_3d/voxel_object_detection_3d_learner.py">3D object detection Voxel tool</a>
whose documentation can be found <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/voxel-object-detection-3d.md">here</a>.</p>

<h4 id="instructions-for-basic-usage-22">Instructions for basic usage:</h4>

<ol>
  <li>
    <p>Start the node responsible for publishing point clouds. OpenDR provides a <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#point-cloud-dataset-ros2-node">point cloud dataset node</a> for convenience.</p>
  </li>
  <li>
    <p>You are then ready to start the 3D object detection node:</p>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ros2 run opendr_perception object_detection_3d_voxel
    
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The following optional arguments are available:    - `-h or --help`: show a help message and exit    - `-i or --input_point_cloud_topic INPUT_POINT_CLOUD_TOPIC`: point cloud topic provided by either a point_cloud_dataset_node or any other 3D point cloud node (default=`/opendr/dataset_point_cloud`)    - `-d or --detections_topic DETECTIONS_TOPIC`: topic name for detection messages (default=`/opendr/objects3d`)    - `--performance_topic PERFORMANCE_TOPIC`: topic name for performance messages (default=`None`, disabled)    - `--device DEVICE`: device to use, either `cpu` or `cuda`, falls back to `cpu` if GPU or CUDA is not found (default=`cuda`)    - `-n or --model_name MODEL_NAME`: name of the trained model (default=`tanet_car_xyres_16`)    - `-c or --model_config_path MODEL_CONFIG_PATH`: path to a model .proto config (default=`../../src/opendr/perception/object_detection3d/voxel_object_detection_3d/second_detector/configs/tanet/car/xyres_16.proto`)
</code></pre></div></div>

<ol>
  <li>Default output topics:
    <ul>
      <li>Detection messages: <code class="language-plaintext highlighter-rouge">/opendr/objects3d</code>
</li>
    </ul>

    <p>For viewing the output, refer to the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#notes">notes above.</a></p>
  </li>
</ol>

<h3 id="3d-object-tracking-ab3dmot-ros2-node">3D Object Tracking AB3DMOT ROS2 Node</h3>

<p>A ROS2 node for performing 3D object tracking using AB3DMOT stateless method.
This is a detection-based method, and therefore the 3D object detector is needed to provide detections, which then will be used to make associations and generate tracking ids.
The predicted tracking annotations are split into two topics with detections and tracking IDs.</p>

<p>You can find the 3D object tracking AB3DMOT ROS2 node python script <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/object_tracking_3d_ab3dmot_node.py">here</a> to inspect the code and modify it as you wish to fit your needs.
The node makes use of the toolkit’s <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/object_tracking_3d/ab3dmot/object_tracking_3d_ab3dmot_learner.py">3D object tracking AB3DMOT tool</a>
whose documentation can be found <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/object-tracking-3d-ab3dmot.md">here</a>.</p>

<h4 id="instructions-for-basic-usage-23">Instructions for basic usage:</h4>

<ol>
  <li>
    <p>Start the node responsible for publishing point clouds. OpenDR provides a <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#point-cloud-dataset-ros2-node">point cloud dataset node</a> for convenience.</p>
  </li>
  <li>
    <p>You are then ready to start the 3D object tracking node:</p>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ros2 run opendr_perception object_tracking_3d_ab3dmot
    
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The following optional arguments are available:    - `-h or --help`: show a help message and exit    - `-i or --input_point_cloud_topic INPUT_POINT_CLOUD_TOPIC`: point cloud topic provided by either a point_cloud_dataset_node or any other 3D point cloud node (default=`/opendr/dataset_point_cloud`)    - `-d or --detections_topic DETECTIONS_TOPIC`: topic name for detection messages, `None` to stop the node from publishing on this topic (default=`/opendr/objects3d`)    - `-t or --tracking3d_id_topic TRACKING3D_ID_TOPIC`: topic name for output tracking IDs with the same element count as in detection topic, `None` to stop the node from publishing on this topic (default=`/opendr/objects_tracking_id`)    - `--performance_topic PERFORMANCE_TOPIC`: topic name for performance messages (default=`None`, disabled)    - `--device DEVICE`: device to use, either `cpu` or `cuda`, falls back to `cpu` if GPU or CUDA is not found (default=`cuda`)    - `-dn or --detector_model_name DETECTOR_MODEL_NAME`: name of the trained model (default=`tanet_car_xyres_16`)    - `-dc or --detector_model_config_path DETECTOR_MODEL_CONFIG_PATH`: path to a model .proto config (default=`../../src/opendr/perception/object_detection3d/voxel_object_detection_3d/second_detector/configs/tanet/car/xyres_16.proto`)
</code></pre></div></div>

<ol>
  <li>Default output topics:
    <ul>
      <li>Detection messages: <code class="language-plaintext highlighter-rouge">/opendr/objects3d</code>
</li>
      <li>Tracking ID messages: <code class="language-plaintext highlighter-rouge">/opendr/objects_tracking_id</code>
</li>
    </ul>

    <p>For viewing the output, refer to the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#notes">notes above.</a></p>
  </li>
</ol>

<h3 id="3d-object-tracking-vpit-ros2-node">3D Object Tracking VPIT ROS2 Node</h3>

<p>A ROS2 node for performing 3D single object tracking using VPIT method.
This method need to be initialized with a 3D bounding box for an object that should be tracked. For this reasone, the initial detection3d box should be sent, as well as the corresponding point cloud. After the initialization, only point cloud data is required for inference. If a new object needs to be tracked, then the same <code class="language-plaintext highlighter-rouge">input_detection3d_topic</code> can be used to send a bounding box and the last send point cloud will be used for initialization.
The predicted tracking annotations are split into two topics with detections and tracking IDs.</p>

<p>You can find the 3D object tracking VPIT ROS2 node python script <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/object_tracking_3d_vpit_node.py">here</a> to inspect the code and modify it as you wish to fit your needs.
The node makes use of the toolkit’s <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/object_tracking_3d/single_object_tracking/vpit/vpit_object_tracking_3d_learner.py">3D object tracking VPIT tool</a>
whose documentation can be found <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/object-tracking-3d-vpit.md">here</a>.</p>

<h4 id="instructions-for-basic-usage-24">Instructions for basic usage:</h4>

<ol>
  <li>
    <p>Start the node responsible for publishing point clouds. OpenDR provides a <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#point-cloud-dataset-ros2-node">point cloud dataset node</a> for convenience.</p>
  </li>
  <li>
    <p>Provide an initial bounding box from either a <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#3d-object-detection-voxel-ros2-node">3D detector</a>, a dataset or a hand-crafted detection 3D box.</p>
  </li>
  <li>
    <p>You are then ready to start the 3D object tracking node:</p>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ros2 run opendr_perception object_tracking_3d_vpit
    
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The following optional arguments are available:    - `-h or --help`: show a help message and exit    - `-ipc or --input_point_cloud_topic INPUT_POINT_CLOUD_TOPIC`: point cloud topic provided by either a point_cloud_dataset_node or any other 3D point cloud node (default=`/opendr/dataset_point_cloud`)    - `-idet or --input_detection3d_topic INPUT_DETECTION3D_TOPIC`: by either a 3D detector or a dataset (default=`/opendr/dataset_detection3d`)    - `-d or --detections_topic DETECTIONS_TOPIC`: topic name for detection messages, `None` to stop the node from publishing on this topic (default=`/opendr/objects3d`)    - `-t or --tracking3d_id_topic TRACKING3D_ID_TOPIC`: topic name for output tracking IDs with the same element count as in detection topic, `None` to stop the node from publishing on this topic (default=`/opendr/objects_tracking_id`)    - `--performance_topic PERFORMANCE_TOPIC`: topic name for performance messages (default=`None`, disabled)    - `--device DEVICE`: device to use, either `cpu` or `cuda`, falls back to `cpu` if GPU or CUDA is not found (default=`cuda`)    - `-bb or --backbone BACKBONE`: Name of the backbone model (default=`pp`, choices=`pp, spp, spps, tanet, stanet, stanets`)    - `-mn or --model_name MODEL_NAME`: Name of the trained model to load (default=`vpit`)
</code></pre></div></div>

<ol>
  <li>Default output topics:
    <ul>
      <li>Detection messages: <code class="language-plaintext highlighter-rouge">/opendr/objects3d</code>
</li>
      <li>Tracking ID messages: <code class="language-plaintext highlighter-rouge">/opendr/objects_tracking_id</code>
</li>
    </ul>

    <p>For viewing the output, refer to the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#notes">notes above.</a></p>
  </li>
</ol>

<h3 id="lidar-based-panoptic-segmentation-ros2-node">LiDAR Based Panoptic Segmentation ROS2 Node</h3>
<p>A ROS node for performing panoptic segmentation on a specified pointcloud stream using the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/panoptic_segmentation/README.md#efficientlps-efficient-lidar-panoptic-segmentation">EfficientLPS</a> network.</p>

<p>You can find the lidar based panoptic segmentation ROS node python script <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/panoptic_segmentation_efficient_lps_node.py">here</a>. You can further also find the point cloud 2 publisher ROS node python script <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/point_cloud_2_publisher_node.py">here</a>, and more explanation <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#point-cloud-2-publisher-ros-node">here</a>.You can inspect the codes and make changes as you wish to fit your needs.
The EfficientLPS node makes use of the toolkit’s <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/panoptic_segmentation/efficient_lps/efficient_lps_learner.py">panoptic segmentation tool</a> whose documentation can be found <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/efficient-lps.md">here</a>
and additional information about EfficientLPS <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/panoptic_segmentation/README.md">here</a>.</p>

<h4 id="instructions-for-basic-usage-25">Instructions for basic usage:</h4>

<ol>
  <li>First one needs to download SemanticKITTI dataset into POINTCLOUD_LOCATION as it is described in the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/panoptic_segmentation/datasets/README.md">Panoptic Segmentation Datasets</a>. Then, once the SPLIT type is specified (train, test or “valid”, default “valid”), the point <strong>Point Cloud 2 Publisher</strong> can be started using the following line:</li>
</ol>

<ul>
  <li>```bash
  ros2 run opendr_perception point_cloud_2_publisher -d POINTCLOUD_LOCATION -s SPLIT</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2. After starting the **PointCloud2 Publisher**, one can start **EfficientLPS Node** using the following line:

- 
```bash
  ros2 run opendr_perception panoptic_segmentation_efficient_lps /opendr/dataset_point_cloud2
  
</code></pre></div></div>

<p>The following optional arguments are available:</p>
<ul>
  <li>
<code class="language-plaintext highlighter-rouge">-h, --help</code>: show a help message and exit</li>
  <li>
<code class="language-plaintext highlighter-rouge">-i or --input_point_cloud_2_topic INPUT_POINTCLOUD2_TOPIC</code> : Point Cloud 2 topic provided by either a point_cloud_2_publisher_node or any other 3D Point Cloud 2 Node (default=<code class="language-plaintext highlighter-rouge">/opendr/dataset_point_cloud2</code>)</li>
  <li>
<code class="language-plaintext highlighter-rouge">--performance_topic PERFORMANCE_TOPIC</code>: topic name for performance messages (default=<code class="language-plaintext highlighter-rouge">None</code>, disabled)</li>
  <li>
<code class="language-plaintext highlighter-rouge">-c or --checkpoint CHECKPOINT</code> : download pretrained models [semantickitti] or load from the provided path (default=<code class="language-plaintext highlighter-rouge">semantickitti</code>)</li>
  <li>
<code class="language-plaintext highlighter-rouge">-o or --output_heatmap_pointcloud_topic OUTPUT_HEATMAP_POINTCLOUD_TOPIC</code>: publish the 3D heatmap pointcloud on <code class="language-plaintext highlighter-rouge">OUTPUT_HEATMAP_POINTCLOUD_TOPIC</code> (default=<code class="language-plaintext highlighter-rouge">/opendr/panoptic</code>)</li>
</ul>

<ol>
  <li>Default output topics:
    <ul>
      <li>Detection messages: <code class="language-plaintext highlighter-rouge">/opendr/panoptic</code>
</li>
    </ul>
  </li>
</ol>

<hr>
<h2 id="biosignal-input">Biosignal input</h2>

<h3 id="heart-anomaly-detection-ros2-node">Heart Anomaly Detection ROS2 Node</h3>

<p>A ROS2 node for performing heart anomaly (atrial fibrillation) detection from ECG data using GRU or ANBOF models trained on AF dataset.</p>

<p>You can find the heart anomaly detection ROS2 node python script <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/heart_anomaly_detection_node.py">here</a> to inspect the code and modify it as you wish to fit your needs.
The node makes use of the toolkit’s heart anomaly detection tools: <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/heart_anomaly_detection/attention_neural_bag_of_feature/attention_neural_bag_of_feature_learner.py">ANBOF tool</a> and
<a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../src/opendr/perception/heart_anomaly_detection/gated_recurrent_unit/gated_recurrent_unit_learner.py">GRU tool</a>, whose documentation can be found here:
<a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/attention-neural-bag-of-feature-learner.md">ANBOF docs</a> and <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/../../../../docs/reference/gated-recurrent-unit-learner.md">GRU docs</a>.</p>

<h4 id="instructions-for-basic-usage-26">Instructions for basic usage:</h4>

<ol>
  <li>
    <p>Start the node responsible for publishing ECG data.</p>
  </li>
  <li>
    <p>You are then ready to start the heart anomaly detection node:</p>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ros2 run opendr_perception heart_anomaly_detection
    
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The following optional arguments are available:    - `-h or --help`: show a help message and exit    - `-i or --input_ecg_topic INPUT_ECG_TOPIC`: topic name for input ECG data (default=`/ecg/ecg`)    - `-o or --output_heart_anomaly_topic OUTPUT_HEART_ANOMALY_TOPIC`: topic name for heart anomaly detection (default=`/opendr/heart_anomaly`)    - `--performance_topic PERFORMANCE_TOPIC`: topic name for performance messages (default=`None`, disabled)    - `--device DEVICE`: device to use, either `cpu` or `cuda`, falls back to `cpu` if GPU or CUDA is not found (default=`cuda`)    - `--model MODEL`: the model to use, choices are `anbof` or `gru` (default=`anbof`)
</code></pre></div></div>

<ol>
  <li>Default output topics:
    <ul>
      <li>Detection messages: <code class="language-plaintext highlighter-rouge">/opendr/heart_anomaly</code>
</li>
    </ul>

    <p>For viewing the output, refer to the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#notes">notes above.</a></p>
  </li>
</ol>

<hr>
<h2 id="dataset-ros2-nodes">Dataset ROS2 Nodes</h2>

<p>The dataset nodes can be used to publish data from the disk, which is useful to test the functionality without the use of a sensor.
Dataset nodes use a provided <code class="language-plaintext highlighter-rouge">DatasetIterator</code> object that returns a <code class="language-plaintext highlighter-rouge">(Data, Target)</code> pair.
If the type of the <code class="language-plaintext highlighter-rouge">Data</code> object is correct, the node will transform it into a corresponding ROS2 message object and publish it to a desired topic.
The OpenDR toolkit currently provides two such nodes, an image dataset node and a point cloud dataset node.</p>

<h3 id="image-dataset-ros2-node">Image Dataset ROS2 Node</h3>

<p>The image dataset node downloads a <code class="language-plaintext highlighter-rouge">nano_MOT20</code> dataset from OpenDR’s FTP server and uses it to publish data to the ROS2 topic,
which is intended to be used with the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#2d-object-tracking-ros2-nodes">2D object tracking nodes</a>.</p>

<p>You can create an instance of this node with any <code class="language-plaintext highlighter-rouge">DatasetIterator</code> object that returns <code class="language-plaintext highlighter-rouge">(Image, Target)</code> as elements,
to use alongside other nodes and datasets.
You can inspect <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/image_dataset_node.py">the node</a> and modify it to your needs for other image datasets.</p>

<p>To get an image from a dataset on the disk, you can start a <code class="language-plaintext highlighter-rouge">image_dataset.py</code> node as:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ros2 run opendr_perception image_dataset

</code></pre></div></div>
<p>The following optional arguments are available:</p>
<ul>
  <li>
<code class="language-plaintext highlighter-rouge">-h or --help</code>: show a help message and exit</li>
  <li>
<code class="language-plaintext highlighter-rouge">-o or --output_rgb_image_topic</code>: topic name to publish the data (default=<code class="language-plaintext highlighter-rouge">/opendr/dataset_image</code>)</li>
  <li>
<code class="language-plaintext highlighter-rouge">-f or --fps FPS</code>: data fps (default=<code class="language-plaintext highlighter-rouge">10</code>)</li>
  <li>
<code class="language-plaintext highlighter-rouge">-d or --dataset_path DATASET_PATH</code>: path to a dataset (default=<code class="language-plaintext highlighter-rouge">/MOT</code>)</li>
  <li>
<code class="language-plaintext highlighter-rouge">-ks or --mot20_subsets_path MOT20_SUBSETS_PATH</code>: path to MOT20 subsets (default=<code class="language-plaintext highlighter-rouge">../../src/opendr/perception/object_tracking_2d/datasets/splits/nano_mot20.train</code>)</li>
</ul>

<h3 id="point-cloud-dataset-ros2-node">Point Cloud Dataset ROS2 Node</h3>

<p>The point cloud dataset node downloads a <code class="language-plaintext highlighter-rouge">nano_KITTI</code> dataset from OpenDR’s FTP server and uses it to publish data to the ROS2 topic,
which is intended to be used with the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#3d-object-detection-voxel-ros2-node">3D object detection node</a>,
as well as the <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#3d-object-tracking-ab3dmot-ros2-node">3D object tracking node</a>.</p>

<p>You can create an instance of this node with any <code class="language-plaintext highlighter-rouge">DatasetIterator</code> object that returns <code class="language-plaintext highlighter-rouge">(PointCloud, Target)</code> as elements,
to use alongside other nodes and datasets.
You can inspect <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/point_cloud_dataset_node.py">the node</a> and modify it to your needs for other point cloud datasets.</p>

<p>To get a point cloud from a dataset on the disk, you can start a <code class="language-plaintext highlighter-rouge">point_cloud_dataset.py</code> node as:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ros2 run opendr_perception point_cloud_dataset

</code></pre></div></div>
<p>The following optional arguments are available:</p>
<ul>
  <li>
<code class="language-plaintext highlighter-rouge">-h or --help</code>: show a help message and exit</li>
  <li>
<code class="language-plaintext highlighter-rouge">-o or --output_point_cloud_topic</code>: topic name to publish the data (default=<code class="language-plaintext highlighter-rouge">/opendr/dataset_point_cloud</code>)</li>
  <li>
<code class="language-plaintext highlighter-rouge">-f or --fps FPS</code>: data fps (default=<code class="language-plaintext highlighter-rouge">10</code>)</li>
  <li>
<code class="language-plaintext highlighter-rouge">-d or --dataset_path DATASET_PATH</code>: path to a dataset, if it does not exist, nano KITTI dataset will be downloaded there (default=<code class="language-plaintext highlighter-rouge">/KITTI/opendr_nano_kitti</code>)</li>
  <li>
<code class="language-plaintext highlighter-rouge">-ks or --kitti_subsets_path KITTI_SUBSETS_PATH</code>: path to KITTI subsets, used only if a KITTI dataset is downloaded (default=<code class="language-plaintext highlighter-rouge">../../src/opendr/perception/object_detection_3d/datasets/nano_kitti_subsets</code>)</li>
</ul>

<h3 id="point-cloud-2-publisher-ros2-node">Point Cloud 2 Publisher ROS2 Node</h3>

<p>The point cloud 2 dataset publisher, publishes point cloud 2 messages from pre-downloaded dataset SemanticKITTI. It is currently being used by the ROS node <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/#lidar-based-panoptic-segmentation-ros-node">LiDAR Based Panoptic Segmentation ROS Node</a>.</p>

<p>You can create an instance of this node with any <code class="language-plaintext highlighter-rouge">DatasetIterator</code> object that returns <code class="language-plaintext highlighter-rouge">(PointCloud, Target)</code> as elements,
to use alongside other nodes and datasets.
You can inspect <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/point_cloud_2_publisher_node.py">the node</a> and modify it to your needs for other point cloud datasets.</p>

<p>To get a point cloud from a dataset on the disk, you can start a <code class="language-plaintext highlighter-rouge">point_cloud_2_publisher_node.py</code> node as:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ros2 run opendr_perception point_cloud_2_publisher

</code></pre></div></div>
<p>The following optional arguments are available:</p>
<ul>
  <li>
<code class="language-plaintext highlighter-rouge">-h or --help</code>: show a help message and exit</li>
  <li>
<code class="language-plaintext highlighter-rouge">-d or --dataset_path DATASET_PATH</code>: path of the SemanticKITTI dataset to publish the point cloud 2 message (default=<code class="language-plaintext highlighter-rouge">./datasets/semantickitti</code>)</li>
  <li>
<code class="language-plaintext highlighter-rouge">-s or --split SPLIT</code>: split of the dataset to use, only (train, valid, test) are available (default=<code class="language-plaintext highlighter-rouge">valid</code>)</li>
  <li>
<code class="language-plaintext highlighter-rouge">-o or --output_point_cloud_2_topic OUTPUT_POINT_CLOUD_2_TOPIC</code>: topic name to publish the data (default=<code class="language-plaintext highlighter-rouge">/opendr/dataset_point_cloud2</code>)</li>
  <li>
<code class="language-plaintext highlighter-rouge">-t or --test_data</code>: Add this argument if you want to only test this node with the test data available in our server</li>
</ul>

<hr>
<h2 id="utility-ros2-nodes">Utility ROS2 Nodes</h2>

<h3 id="performance-ros2-node">Performance ROS2 Node</h3>

<p>The performance node is used to subscribe to the optional performance topic of a running node and log its performance in terms of the time it
took to process a single input and produce output and in terms of frames per second. It uses a modifiable rolling window to calculate the average FPS.</p>

<p>You can inspect <a href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception/./opendr_perception/performance_node.py">the node</a> and modify it to your needs.</p>

<h4 id="instructions-for-basic-usage-27">Instructions for basic usage:</h4>

<ol>
  <li>Start the node you want to benchmark as usual but also set the optional argument <code class="language-plaintext highlighter-rouge">--performance_topic</code> to, for example, <code class="language-plaintext highlighter-rouge">/opendr/performance</code>
</li>
  <li>Start the performance node:</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ros2 run opendr_perception performance
    
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The following optional arguments are available:    - `-h or --help`: show a help message and exit    - `-i or --input_performance_topic INPUT_PERFORMANCE_TOPIC`: topic name for input performance data (default=`/opendr/performance`)    - `-w or --window WINDOW`: the window to use in number of frames to calculate the running average FPS (default=`20`)
</code></pre></div></div>

<p>Note that the <code class="language-plaintext highlighter-rouge">input_performance_topic</code> of the performance node must match the <code class="language-plaintext highlighter-rouge">performance_topic</code> of the running node.
Also note that the running node should properly get input and produce output to publish performance messages for the performance node to use.</p>
</div>
              </div>
            </div>
          </div>
        </div>
      
    
    <div class="row">
      <div class="col-xs-12">
        <div class="panel panel-default">
          <div class="panel-heading"><span class="glyphicon glyphicon-list"></span> CHANGELOG</div>
          <div class="panel-body">
            
              <em>No CHANGELOG found.</em>
            
          </div>
        </div>
      </div>
    </div>
  </div>

  <div class="tab-pane" id="github-tutorials">
    <div class="panel panel-default">
      <div class="panel-heading">
        <h3 class="panel-title">Wiki Tutorials</h3>
      </div>
      <div class="panel-body">
        
        <em>This package does not provide any links to tutorials in it's <a href="https://index.ros.org/contribute/metadata/">rosindex metadata</a>.</em>
        <em>You can check on the <a href="http://wiki.ros.org/opendr_perception/Tutorials" target="_blank">ROS Wiki Tutorials</a> page for the package.</em>
        
      </div>
    </div>
  </div>

  <div class="tab-pane" id="github-deps">
    <div class="row">
      <div class="col-sm-6">
        <div class="panel panel-default">
          <div class="panel-heading">
            <h3 class="panel-title">Package Dependencies</h3>
          </div>
          <div class="panel-body">
            
              <table class="table table-condensed table-striped">
                <thead>
                  <th class="text-center">Deps</th>
                  <th style="width: 100%">Name</th>
                </thead>
                <tbody>
                  
                    
                    
                      <tr>
                        <td class="text-center">
                          
                          <a href="/p/rclpy#github-deps" class="endoftree">
                            <span class="glyphicon glyphicon-arrow-left" title="Package dependencies"></span>
                          </a>
                        </td>
                        <td><a href="/p/rclpy#github">rclpy  </a></td>
                      </tr>
                    
                  
                    
                    
                      <tr>
                        <td class="text-center">
                          
                          <a href="/p/std_msgs#github-deps" >
                            <span class="glyphicon glyphicon-arrow-left" title="Package dependencies"></span>
                          </a>
                        </td>
                        <td><a href="/p/std_msgs#github">std_msgs  </a></td>
                      </tr>
                    
                  
                    
                    
                      <tr>
                        <td class="text-center">
                          
                          <a href="/p/vision_msgs#github-deps" >
                            <span class="glyphicon glyphicon-arrow-left" title="Package dependencies"></span>
                          </a>
                        </td>
                        <td><a href="/p/vision_msgs#github">vision_msgs  </a></td>
                      </tr>
                    
                  
                    
                    
                      <tr>
                        <td class="text-center">
                          
                          <a href="/p/geometry_msgs#github-deps" >
                            <span class="glyphicon glyphicon-arrow-left" title="Package dependencies"></span>
                          </a>
                        </td>
                        <td><a href="/p/geometry_msgs#github">geometry_msgs  </a></td>
                      </tr>
                    
                  
                    
                    
                      <tr>
                        <td class="text-center">
                          
                          <a href="/p/opendr_bridge#github-deps" >
                            <span class="glyphicon glyphicon-arrow-left" title="Package dependencies"></span>
                          </a>
                        </td>
                        <td><a href="/p/opendr_bridge#github">opendr_bridge  </a></td>
                      </tr>
                    
                  
                    
                    
                      <tr>
                        <td class="text-center">
                          
                          <a href="/p/ament_copyright#github-deps" class="endoftree">
                            <span class="glyphicon glyphicon-arrow-left" title="Package dependencies"></span>
                          </a>
                        </td>
                        <td><a href="/p/ament_copyright#github">ament_copyright  </a></td>
                      </tr>
                    
                  
                    
                    
                      <tr>
                        <td class="text-center">
                          
                          <a href="/p/ament_flake8#github-deps" class="endoftree">
                            <span class="glyphicon glyphicon-arrow-left" title="Package dependencies"></span>
                          </a>
                        </td>
                        <td><a href="/p/ament_flake8#github">ament_flake8  </a></td>
                      </tr>
                    
                  
                    
                    
                      <tr>
                        <td class="text-center">
                          
                          <a href="/p/ament_pep257#github-deps" class="endoftree">
                            <span class="glyphicon glyphicon-arrow-left" title="Package dependencies"></span>
                          </a>
                        </td>
                        <td><a href="/p/ament_pep257#github">ament_pep257  </a></td>
                      </tr>
                    
                  
                </tbody>
              </table>
            
          </div>
          <div class="panel-heading">
            <h3 class="panel-title">System Dependencies</h3>
          </div>
          <div class="panel-body">
            
              <table class="table table-condensed table-striped">
                <thead>
                  <th>Name</th>
                </thead>
                <tbody>
                  
                    
                      <tr>
                        <td><a href="/d/python3-pytest">python3-pytest</a></td>
                      </tr>
                    
                  
                </tbody>
              </table>
            
          </div>
        </div>
      </div>

      <div class="col-sm-6">
        <div class="panel panel-default">
          <div class="panel-heading">
            <h3 class="panel-title">Dependant Packages</h3>
          </div>
          <div class="panel-body">
            
              <em>No known dependants.</em>
            
          </div>
        </div>
      </div>
    </div>
  </div>

  <div class="tab-pane" id="github-assets">
    <div class="panel panel-default">
      <div class="panel-heading">
        <h3 class="panel-title">Launch files</h3>
      </div>
      <div class="panel-body">
        
          <em>No launch files found</em>
        
      </div>
    </div>

    <div class="panel panel-default">
      <div class="panel-heading">
        <h3 class="panel-title">Messages</h3>
      </div>
      <div class="panel-body">
        
          <em>No message files found.</em>
        
      </div>
    </div>

    <div class="panel panel-default">
      <div class="panel-heading">
        <h3 class="panel-title">Services</h3>
      </div>
      <div class="panel-body">
        
          <em>No service files found</em>
        
      </div>
    </div>

    <div class="panel panel-default">
      <div class="panel-heading">
        <h3 class="panel-title">Plugins</h3>
      </div>
      <div class="panel-body">
        
          <em>No plugins found.</em>
        
      </div>
    </div>
  </div>

  <div class="tab-pane" id="github-questions">
    <div class="panel panel-default">
      <div class="panel-heading">
        <h3 class="panel-title">Recent questions tagged <kbd>opendr_perception</kbd> at <strong><a href="https://robotics.stackexchange.com/" target="_blank">Robotics Stack Exchange</a></strong></h3>      </div>
      <div id="github-question-list" class="panel-body" style="display: none;"></div>
      <div id="github-no-question-list" class="panel-body" style="display: none;">
        <p>No questions yet, you can ask one <a href="https://robotics.stackexchange.com/questions/tagged/opendr_perception+github">here</a>.</p>
      </div>
      <div id="github-get-question-fail" class="panel-body alert alert-warning" style="display: none;">
        <p>Failed to get question list, you can ticket an issue <a href="https://github.com/ros-infrastructure/rosindex/issues/new" target="_blank" class="alert-link">here</a> </p>
      </div>
    </div>
  </div>
</div>

<script type="text/javascript" src="/js/package_body_tabs.js"></script>

<script src=/js/contribution_suggestions.js></script>
<script type="text/javascript">
jQuery(function() {
    // Update the count on the links to the contribution suggestions
    updateContributionSuggestionsCountOnPackage('https://github.com/opendr-eu/opendr.git');
});
</script>

          </div>
          <div class="col-md-2 hidden-xs hidden-sm">
            
            
            



<div class="list-group list-group-sm ">
  <a class="list-group-item"
     target="_blank"
     href="http://docs.ros.org/en/github/p/opendr_perception"
     title="View API documentation on docs.ros.org"
     >
       <span style="margin-right: 5px;" class="glyphicon glyphicon-file"></span>
       
       API Docs
  </a>
  <a class="list-group-item"
     target="_blank"
     href="https://github.com/opendr-eu/opendr/tree/master/projects/opendr_ws_2/src/opendr_perception"
     title="View source code on repository">
       <span class="glyphicon glyphicon-folder-open" style="margin-right:5px;"></span>
       
       Browse Code
  </a>
  
  
  
  
  
  
</div>

          </div>
        </div>
      
    </div>
  </div>

  <div class="distro distro-noetic">
    <div class="container-fluid">
      
          <div class="alert alert-warning">No version for distro <strong>noetic</strong>. Known supported distros are highlighted in the buttons above.</div>
      
    </div>
  </div>

  <div class="distro distro-galactic">
    <div class="container-fluid">
      
          <div class="alert alert-warning">No version for distro <strong>galactic</strong>. Known supported distros are highlighted in the buttons above.</div>
      
    </div>
  </div>

  <div class="distro distro-iron">
    <div class="container-fluid">
      
          <div class="alert alert-warning">No version for distro <strong>iron</strong>. Known supported distros are highlighted in the buttons above.</div>
      
    </div>
  </div>

  <div class="distro distro-melodic">
    <div class="container-fluid">
      
          <div class="alert alert-warning">No version for distro <strong>melodic</strong>. Known supported distros are highlighted in the buttons above.</div>
      
    </div>
  </div>



<script id="question-list-template" type="text/html">
  <p>
    Browse <a href="https://robotics.stackexchange.com/questions/tagged/opendr_perception?tab=Newest">recent questions</a>
    or see the <a href="https://robotics.stackexchange.com/questions/tagged/opendr_perception?tab=Votes">highest voted ones</a>.
  </p>
  
  <table class="table table-striped">
    <thead>
      <tr>
        <th scope="col">Title</th>
        <th scope="col">Answered?</th>
        <th scope="col">Created</th>
        <th scope="col">Active</th>
      </tr>
    </thead>
    <tbody>
      {{#questions}}
      <tr>
        <th scope="row"><a href="{{link}}">{{title}}</a><br><small>Tags: {{strtags}}</small></th>
        <th>{{answered}}</th>
        <th>{{created}}</th>
        <th>{{active}}</th>
      </tr>
      {{/questions}}
    </tbody>
  </table>
  {{#has_more}}
  <hr>
  <h4>Additional questions are available at
    <a href="https://robotics.stackexchange.com/questions/tagged/{{name}}?tab=Newest">Robotics Stack Exchange</a>
  </h4>
  {{/has_more}}
  
</script>

<script type="text/javascript" src="/js/mustache.js"></script>
<script type="text/javascript">
  jQuery(function() {
    jQuery.getJSON(url="https://api.stackexchange.com/2.3/questions?order=desc&site=robotics" +
                       "&tagged=opendr_perception&pagesize=50&sort=activity")
    .done(function(feed) {
      if (feed.items.length > 0) {
        var question_count_str = (feed.has_more ? '>' : '') + String(feed.items.length)
        const questions = feed.items.map(
          function(question) {
            question.id = Math.round(Math.random() * 99999.0);
            question.created = (new Date(question.creation_date * 1000)).toDateString()
            question.active = (new Date(question.last_activity_date * 1000)).toDateString()
            question.answered = question.is_answered ? "Yes" : "No"
            question.strtags = question.tags.join(' ')
            return question;
          });
        const template = jQuery("#question-list-template").text();
        Mustache.parse(template);
        var rendered = Mustache.render(
          template, 
          { questions: questions,
            has_more: feed.has_more,
            name: "opendr_perception"
          }
        );
      }
      //   TODO(tfoote) Resolve images
      //   jQuery("#-question-list img").each(function() {
      //     if (jQuery(this).attr("src").startsWith("/upfiles")) {
      //        jQuery(this).attr("src", "https://answers.ros.org" + jQuery(this).attr("src"));
      //     }
      //   });
      
      if (feed.items.length > 0) {
        jQuery("#humble-question-list").html(rendered);
        jQuery("#humble-question-list").show();
      } else {
        jQuery("#humble-no-question-list").show();
      }
      jQuery("#humble-questions-count").text(question_count_str);
      
      if (feed.items.length > 0) {
        jQuery("#jazzy-question-list").html(rendered);
        jQuery("#jazzy-question-list").show();
      } else {
        jQuery("#jazzy-no-question-list").show();
      }
      jQuery("#jazzy-questions-count").text(question_count_str);
      
      if (feed.items.length > 0) {
        jQuery("#kilted-question-list").html(rendered);
        jQuery("#kilted-question-list").show();
      } else {
        jQuery("#kilted-no-question-list").show();
      }
      jQuery("#kilted-questions-count").text(question_count_str);
      
      if (feed.items.length > 0) {
        jQuery("#rolling-question-list").html(rendered);
        jQuery("#rolling-question-list").show();
      } else {
        jQuery("#rolling-no-question-list").show();
      }
      jQuery("#rolling-questions-count").text(question_count_str);
      
      if (feed.items.length > 0) {
        jQuery("#github-question-list").html(rendered);
        jQuery("#github-question-list").show();
      } else {
        jQuery("#github-no-question-list").show();
      }
      jQuery("#github-questions-count").text(question_count_str);
      
      if (feed.items.length > 0) {
        jQuery("#noetic-question-list").html(rendered);
        jQuery("#noetic-question-list").show();
      } else {
        jQuery("#noetic-no-question-list").show();
      }
      jQuery("#noetic-questions-count").text(question_count_str);
      
      if (feed.items.length > 0) {
        jQuery("#galactic-question-list").html(rendered);
        jQuery("#galactic-question-list").show();
      } else {
        jQuery("#galactic-no-question-list").show();
      }
      jQuery("#galactic-questions-count").text(question_count_str);
      
      if (feed.items.length > 0) {
        jQuery("#iron-question-list").html(rendered);
        jQuery("#iron-question-list").show();
      } else {
        jQuery("#iron-no-question-list").show();
      }
      jQuery("#iron-questions-count").text(question_count_str);
      
      if (feed.items.length > 0) {
        jQuery("#melodic-question-list").html(rendered);
        jQuery("#melodic-question-list").show();
      } else {
        jQuery("#melodic-no-question-list").show();
      }
      jQuery("#melodic-questions-count").text(question_count_str);
      
    })
    .fail(function(jqxhr, textStatus, error) {
      console.error("Failed to get question data: " + jqxhr.responseText);
      
      jQuery("#humble-get-question-fail").show();
      
      jQuery("#jazzy-get-question-fail").show();
      
      jQuery("#kilted-get-question-fail").show();
      
      jQuery("#rolling-get-question-fail").show();
      
      jQuery("#github-get-question-fail").show();
      
      jQuery("#noetic-get-question-fail").show();
      
      jQuery("#galactic-get-question-fail").show();
      
      jQuery("#iron-get-question-fail").show();
      
      jQuery("#melodic-get-question-fail").show();
      
    });
  });
</script>


<script type="text/javascript">
  $(document).ready(function() {
    setupDistroSwitch("humble");
  });
</script>

      </div>
    </div>

    <footer class="site-footer">
  <div class="wrapper">
    <div class="container-fluid">
      <div style="float:left;">
        
          <a href="https://github.com/rkent/rosindex" title="Find rosindex in Github">
          <span class="icon  icon--github">
            <svg viewBox="0 0 16 16">
              <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
            </svg>
          </span>

          <span class="username">rkent/rosindex</span>
        </a>
        <em class="hidden-xs">| generated on 2025-05-05</em>
      
      </div>
      <div style="float:right;">
        <p class="text"><span class="hidden-xs">a community-maintained index of robotics software
 | </span><a href="/privacy.txt">privacy</a></p>
      </div>
    </div>
  </div>

</footer>


  </body>

</html>
